{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f24f7146",
      "metadata": {
        "id": "f24f7146"
      },
      "source": [
        "Target Specification:\n",
        "- OS: Ubuntu 22.04.4 LTS\n",
        "- Python: 3.12\n",
        "- Platform: Google Colab\n",
        "- CPU: Intel Xeon E5â€‘2699 v4 1 Core / 2 Thread\n",
        "- RAM: 13GB\n",
        "- GPU: Nvidia T4 15GB (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e29cbc29",
      "metadata": {
        "id": "e29cbc29"
      },
      "source": [
        "Library Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2675fdef",
      "metadata": {
        "id": "2675fdef"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import math\n",
        "import glob\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import torch.optim as optim\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torch.amp import GradScaler, autocast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torchvision.transforms import ToTensor, ToPILImage, Compose, ColorJitter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JijEE4vT3O54",
      "metadata": {
        "id": "JijEE4vT3O54"
      },
      "source": [
        "Pre Training Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kJzDUghQ3PUg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJzDUghQ3PUg",
        "outputId": "6537910a-568d-4eec-9d8b-2a2d2de14d09"
      },
      "outputs": [],
      "source": [
        "%pip install pytorch-msssim onnxruntime-gpu\n",
        "from pytorch_msssim import ssim\n",
        "\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"Google Drive is already mounted.\")\n",
        "\n",
        "if not os.path.exists('/content/datasets'):\n",
        "    print(\"Copying Datasets...\")\n",
        "    !sudo cp -rf /content/drive/MyDrive/Thesis/datasets /content/datasets\n",
        "else:\n",
        "    print(\"datasets already exist.\")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "if DEVICE == \"cuda\":\n",
        "    # %pip install tensorrt onnxruntime-gpu\n",
        "    import onnxruntime as ort\n",
        "    # We have a T4 (16GB VRAM), let's use it\n",
        "    BATCH_SIZE = 128\n",
        "    print(\"CUDA (T4 GPU) detected. Using BATCH_SIZE=128.\")\n",
        "else:\n",
        "    # %pip install onnxruntime\n",
        "    import onnxruntime as ort\n",
        "    # We are on CPU-only (13GB shared RAM), be more conservative\n",
        "    BATCH_SIZE = 64\n",
        "    print(\"No GPU detected. Using CPU-only with BATCH_SIZE=64.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09162a59",
      "metadata": {
        "id": "09162a59"
      },
      "source": [
        "Training Config & Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "731e96d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "731e96d0",
        "outputId": "282b4bb6-abd0-47f2-deb1-7c9d2f9b97f9"
      },
      "outputs": [],
      "source": [
        "UPSCALE_FACTOR = 2\n",
        "NUM_WORKER = os.cpu_count()\n",
        "EPOCHS = 150\n",
        "PATCH_SIZE = 256\n",
        "LEARNING_RATE = 1e-4\n",
        "LOSS_ALPHA = 0.84\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/Thesis\")\n",
        "DATA_DIR = Path(\"/content/datasets\")\n",
        "OUTPUTS_DIR = BASE_DIR / \"outputs\"\n",
        "CHECKPOINTS_DIR = BASE_DIR / \"checkpoints\"\n",
        "CHECKPOINT_FILE = CHECKPOINTS_DIR / f\"best_espcn_x{UPSCALE_FACTOR}.pth\"\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"Enabling cuDNN benchmark mode for GPU.\")\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "806b70ea",
      "metadata": {
        "id": "806b70ea"
      },
      "source": [
        "Environment Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d4d01f6",
      "metadata": {
        "id": "4d4d01f6"
      },
      "outputs": [],
      "source": [
        "def prepare_environment_and_datasets(patch_size):\n",
        "    CHECKPOINTS_DIR.mkdir(exist_ok=True)\n",
        "    OUTPUTS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "    # Define a path for the cache file\n",
        "    CACHE_FILE_TRAIN = OUTPUTS_DIR / \"valid_train_paths.txt\"\n",
        "\n",
        "    # --- Handle Validation Paths (always scanned, it's fast) ---\n",
        "    valid_div2k_dir = DATA_DIR / \"DIV2K_valid_HR\"\n",
        "    validation_image_paths = glob.glob(str(valid_div2k_dir / '*.*'))\n",
        "\n",
        "    # --- Handle Training Paths (Check cache first) ---\n",
        "    valid_train_paths = []\n",
        "    if CACHE_FILE_TRAIN.exists():\n",
        "        print(f\"Loading cached training image paths from {CACHE_FILE_TRAIN}...\")\n",
        "        try:\n",
        "            with open(CACHE_FILE_TRAIN, 'r') as f:\n",
        "                valid_train_paths = [line.strip() for line in f if line.strip()]\n",
        "            print(f\"Loaded {len(valid_train_paths)} paths from cache.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not read cache file {e}. Re-building...\")\n",
        "            valid_train_paths = [] # Ensure list is empty to trigger re-build\n",
        "\n",
        "    if not valid_train_paths:\n",
        "        print(f\"Cache file not found or was invalid. Building new image list...\")\n",
        "        train_div2k_dir = DATA_DIR / \"DIV2K_train_HR\"\n",
        "        train_flickr2k_dir = DATA_DIR / \"Flickr2K_HR\"\n",
        "        personal_dir = DATA_DIR / \"Personal_HR\"\n",
        "\n",
        "        train_image_paths = glob.glob(str(train_div2k_dir / '*.*')) + \\\n",
        "                            glob.glob(str(train_flickr2k_dir / '*.*'))\n",
        "\n",
        "        if personal_dir.exists():\n",
        "            print(\"Personal dataset found. Adding to training set.\")\n",
        "            train_image_paths += glob.glob(str(personal_dir / '*.*'))\n",
        "        else:\n",
        "            print(\"Personal dataset not found. Proceeding without it.\")\n",
        "\n",
        "        print(f\"Found {len(train_image_paths)} potential training images.\")\n",
        "        print(f\"Verifying image dimensions against patch size {patch_size}...\")\n",
        "\n",
        "        def is_image_large_enough(image_path, min_size):\n",
        "            try:\n",
        "                img = cv2.imread(str(image_path))\n",
        "                if img is None:\n",
        "                    print(f\"Warning: Failed to load {image_path}. Skipping.\")\n",
        "                    return False\n",
        "                h, w = img.shape[:2]\n",
        "                return h >= min_size and w >= min_size\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error reading {image_path}: {e}. Skipping.\")\n",
        "                return False\n",
        "\n",
        "        valid_train_paths = [\n",
        "            p for p in tqdm(train_image_paths, desc=\"Filtering train images\")\n",
        "            if is_image_large_enough(p, patch_size)\n",
        "        ]\n",
        "\n",
        "        print(f\"Filtered training set: {len(valid_train_paths)} of {len(train_image_paths)} images remain.\")\n",
        "\n",
        "        # Write the new valid paths to the cache file\n",
        "        try:\n",
        "            print(f\"Saving new cache file to {CACHE_FILE_TRAIN}...\")\n",
        "            with open(CACHE_FILE_TRAIN, 'w') as f:\n",
        "                for path in valid_train_paths:\n",
        "                    f.write(f\"{path}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not write cache file: {e}\")\n",
        "\n",
        "    if not valid_train_paths or not validation_image_paths:\n",
        "        print(\"\\n--- VERIFICATION FAILED: No images found. Check your './datasets/' folder structure. ---\")\n",
        "\n",
        "    return valid_train_paths, validation_image_paths"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0a1cbcd",
      "metadata": {
        "id": "a0a1cbcd"
      },
      "source": [
        "Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85da2e21",
      "metadata": {
        "id": "85da2e21"
      },
      "outputs": [],
      "source": [
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.7):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        try:\n",
        "            self.ssim_loss_fn = ssim\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install pytorch-msssim: pip install pytorch-msssim\")\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        # MSE Loss\n",
        "        loss_mse = self.mse_loss(output, target)\n",
        "\n",
        "        # SSIM Loss (1 - SSIM)\n",
        "        ssim_score = self.ssim_loss_fn(output, target, data_range=1.0, size_average=True)\n",
        "        loss_ssim = 1 - ssim_score\n",
        "\n",
        "        # Combined Loss\n",
        "        total_loss = self.alpha * loss_mse + (1 - self.alpha) * loss_ssim\n",
        "\n",
        "        return total_loss, loss_mse, ssim_score\n",
        "\n",
        "def psnr(mse):\n",
        "    return 10 * math.log10(1 / mse) if mse > 0 else float('inf')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6698a64d",
      "metadata": {
        "id": "6698a64d"
      },
      "source": [
        "Create Patches From HR Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f942143",
      "metadata": {
        "id": "2f942143"
      },
      "outputs": [],
      "source": [
        "class TrainingSuperResolutionDataset(Dataset):\n",
        "    def __init__(self, image_filenames, crop_size, upscale_factor):\n",
        "        super(TrainingSuperResolutionDataset, self).__init__()\n",
        "        self.image_filenames = image_filenames\n",
        "        self.crop_size = crop_size - (crop_size % upscale_factor)\n",
        "        self.upscale_factor = upscale_factor\n",
        "        self.to_tensor = ToTensor()\n",
        "        self.scale_factor = 1 / upscale_factor\n",
        "        self.transform = Compose([\n",
        "            ToPILImage(),\n",
        "            ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "            ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        try:\n",
        "            hr_image = cv2.imread(self.image_filenames[index])\n",
        "            if hr_image is None:\n",
        "                raise IOError(f\"cv2.imread failed to load image: {self.image_filenames[index]}\")\n",
        "            hr_image = cv2.cvtColor(hr_image, cv2.COLOR_BGR2RGB)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {self.image_filenames[index]}: {e}. Skipping.\")\n",
        "            return self.__getitem__((index + 1) % len(self))\n",
        "\n",
        "        h, w = hr_image.shape[:2]\n",
        "\n",
        "        i = random.randint(0, h - self.crop_size)\n",
        "        j = random.randint(0, w - self.crop_size)\n",
        "        hr_patch = hr_image[i:i+self.crop_size, j:j+self.crop_size, :]\n",
        "\n",
        "        if torch.rand(1) > 0.5:\n",
        "            hr_patch = cv2.flip(hr_patch, 1)\n",
        "\n",
        "        if torch.rand(1) > 0.5:\n",
        "            angle = random.choice([cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_180, cv2.ROTATE_90_COUNTERCLOCKWISE])\n",
        "            hr_patch = cv2.rotate(hr_patch, angle)\n",
        "\n",
        "        # --- START: Recommended Change ---\n",
        "\n",
        "        # 1. Create the CLEAN HR (target) tensor first\n",
        "        hr_patch_tensor = self.transform(hr_patch)\n",
        "\n",
        "        # 2. Create the CLEAN LR tensor from the clean HR tensor\n",
        "        lr_patch_tensor = F.interpolate(\n",
        "            hr_patch_tensor.unsqueeze(0),\n",
        "            scale_factor=self.scale_factor,\n",
        "            mode='bicubic',\n",
        "            align_corners=False,\n",
        "            antialias=True\n",
        "        ).squeeze(0)\n",
        "\n",
        "        # --- END: Recommended Change ---\n",
        "\n",
        "        return lr_patch_tensor, hr_patch_tensor # (Augmented_LR, Clean_HR)\n",
        "\n",
        "class ValidationSuperResolutionDataset(Dataset):\n",
        "    def __init__(self, image_filenames, upscale_factor):\n",
        "        super(ValidationSuperResolutionDataset, self).__init__()\n",
        "        self.image_filenames = image_filenames\n",
        "        self.upscale_factor = upscale_factor\n",
        "        self.to_tensor = ToTensor()\n",
        "        self.scale_factor = 1 / upscale_factor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        try:\n",
        "            hr_image = cv2.imread(self.image_filenames[index])\n",
        "            if hr_image is None:\n",
        "                raise IOError(f\"cv2.imread failed to load image: {self.image_filenames[index]}\")\n",
        "            hr_image = cv2.cvtColor(hr_image, cv2.COLOR_BGR2RGB)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {self.image_filenames[index]}: {e}. Skipping.\")\n",
        "            return self.__getitem__((index + 1) % len(self))\n",
        "\n",
        "        h, w = hr_image.shape[:2]\n",
        "        w_new, h_new = w - (w % self.upscale_factor), h - (h % self.upscale_factor)\n",
        "        hr_image = hr_image[:h_new, :w_new, :]\n",
        "\n",
        "        hr_tensor = self.to_tensor(hr_image)\n",
        "\n",
        "        # This is the \"interpolasi bikubik\" method from your proposal\n",
        "        lr_tensor = F.interpolate(\n",
        "            hr_tensor.unsqueeze(0),\n",
        "            scale_factor=self.scale_factor,\n",
        "            mode='bicubic',\n",
        "            align_corners=False,\n",
        "            antialias=True\n",
        "        ).squeeze(0)\n",
        "\n",
        "        return lr_tensor, hr_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9e70967",
      "metadata": {
        "id": "a9e70967"
      },
      "source": [
        "Model Defintion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e5ccae0",
      "metadata": {
        "id": "3e5ccae0"
      },
      "outputs": [],
      "source": [
        "class ESPCN(nn.Module):\n",
        "    def __init__(self, upscale_factor):\n",
        "        super(ESPCN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 3 * (upscale_factor ** 2), kernel_size=3, padding=1)\n",
        "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
        "        self.relu = nn.ReLU() \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pixel_shuffle(self.conv3(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d250a4ae",
      "metadata": {
        "id": "d250a4ae"
      },
      "source": [
        "Execute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197dcee6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "197dcee6",
        "outputId": "de2824fc-e5f4-4669-e3b5-fbb0877f8721"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    train_paths, val_paths = prepare_environment_and_datasets(PATCH_SIZE)\n",
        "\n",
        "    if not train_paths or not val_paths:\n",
        "        raise RuntimeError(\"Dataset paths not found. Halting execution.\")\n",
        "\n",
        "    train_dataset = TrainingSuperResolutionDataset(train_paths, crop_size=PATCH_SIZE, upscale_factor=UPSCALE_FACTOR)\n",
        "    val_dataset = ValidationSuperResolutionDataset(val_paths, upscale_factor=UPSCALE_FACTOR)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=min(4, NUM_WORKER),\n",
        "        pin_memory=(DEVICE == \"cuda\"),\n",
        "        persistent_workers=True,\n",
        "        worker_init_fn=seed_worker\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=1, shuffle=False,\n",
        "        num_workers=min(4, NUM_WORKER),\n",
        "        pin_memory=(DEVICE == \"cuda\"),\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "    print(f\"Found {len(train_dataset)} training images and {len(val_dataset)} validation images.\")\n",
        "\n",
        "    # Model initialization and multi-GPU support\n",
        "    model = ESPCN(upscale_factor=UPSCALE_FACTOR)\n",
        "    if DEVICE == 'cuda' and torch.cuda.device_count() > 1:\n",
        "        print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel.\")\n",
        "        model = torch.nn.DataParallel(model)\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    criterion = CombinedLoss(alpha=LOSS_ALPHA).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
        "    scaler = GradScaler(enabled=(DEVICE == \"cuda\"))\n",
        "\n",
        "    # --- Checkpoint Loading Logic ---\n",
        "    start_epoch = 0\n",
        "    best_psnr = 0.0\n",
        "    log_file_path = OUTPUTS_DIR / 'training_log.csv'\n",
        "    training_log = []\n",
        "\n",
        "    if CHECKPOINT_FILE.exists():\n",
        "        print(f\"Resuming training from checkpoint: {CHECKPOINT_FILE}\")\n",
        "        checkpoint = torch.load(CHECKPOINT_FILE, map_location=DEVICE)\n",
        "        original_state_dict = checkpoint['model_state_dict']\n",
        "        new_state_dict = {}\n",
        "\n",
        "        # Handle '_orig_mod.' (torch.compile) prefix gracefully\n",
        "        needs_prefix_stripping = any(key.startswith('_orig_mod.') for key in original_state_dict.keys())\n",
        "        if needs_prefix_stripping:\n",
        "            print(\"Detected '_orig_mod.' prefix from torch.compile(). Stripping prefix...\")\n",
        "            for key, value in original_state_dict.items():\n",
        "                if key.startswith('_orig_mod.'):\n",
        "                    new_key = key[len('_orig_mod.'):]  # Remove the prefix\n",
        "                    new_state_dict[new_key] = value\n",
        "                else:\n",
        "                    new_state_dict[key] = value\n",
        "        else:\n",
        "            print(\"No '_orig_mod.' prefix detected. Loading state dict as is.\")\n",
        "            new_state_dict = original_state_dict\n",
        "\n",
        "        # Handle legacy 'module.' DataParallel prefix (optional, safe for all cases)\n",
        "        needs_module_prefix_stripping = any(key.startswith('module.') for key in new_state_dict.keys())\n",
        "        if needs_module_prefix_stripping:\n",
        "            print(\"Detected 'module.' prefix from DataParallel. Stripping prefix...\")\n",
        "            stripped_dict = {}\n",
        "            for key, value in new_state_dict.items():\n",
        "                if key.startswith('module.'):\n",
        "                    new_key = key[len('module.'):]\n",
        "                    stripped_dict[new_key] = value\n",
        "                else:\n",
        "                    stripped_dict[key] = value\n",
        "            new_state_dict = stripped_dict\n",
        "\n",
        "        model.load_state_dict(new_state_dict)\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        best_psnr = checkpoint.get('best_psnr', 0.0)\n",
        "        print(f\"Resumed from epoch {start_epoch}, with best PSNR of {best_psnr:.4f} dB.\")\n",
        "\n",
        "        # Load existing log if it exists\n",
        "        if log_file_path.exists():\n",
        "            try:\n",
        "                print(f\"Loading existing training log from {log_file_path}\")\n",
        "                log_df = pd.read_csv(log_file_path)\n",
        "                log_df = log_df[log_df['epoch'] < start_epoch]\n",
        "                training_log = log_df.to_dict('records')\n",
        "                print(f\"Loaded {len(training_log)} previous log entries.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not load or parse log file: {e}. Starting with an empty log.\")\n",
        "                training_log = []\n",
        "        else:\n",
        "            print(\"No existing log file found. Starting a new log.\")\n",
        "            training_log = []\n",
        "    else:\n",
        "        print(\"No checkpoint found. Starting training from scratch.\")\n",
        "        training_log = []\n",
        "        if log_file_path.exists():\n",
        "            print(f\"Deleting old log file: {log_file_path}\")\n",
        "            log_file_path.unlink()\n",
        "\n",
        "    # --- Now torch.compile (AFTER checkpoint loading) ---\n",
        "    try:\n",
        "        print(f\"Compiling model with torch.compile for {DEVICE}...\")\n",
        "        model = torch.compile(model)\n",
        "    except Exception as e:\n",
        "        print(f\"torch.compile() for {DEVICE} failed: {e}. Running in eager mode.\")\n",
        "\n",
        "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"\\n--- Training ESPCN Model ({num_params:,} parameters) ---\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(start_epoch, EPOCHS):\n",
        "        model.train()\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "        for lr_images, hr_images in progress_bar:\n",
        "            lr_images = lr_images.to(DEVICE, non_blocking=(DEVICE == \"cuda\"))\n",
        "            hr_images = hr_images.to(DEVICE, non_blocking=(DEVICE == \"cuda\"))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if DEVICE == \"cuda\":\n",
        "                with autocast(device_type=DEVICE):\n",
        "                    outputs = model(lr_images)\n",
        "                total_loss, mse, ssim_score = criterion(outputs.float(), hr_images.float())\n",
        "                scaler.scale(total_loss).backward()\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                outputs = model(lr_images)\n",
        "                total_loss, mse, ssim_score = criterion(outputs.float(), hr_images.float())\n",
        "                total_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "            progress_bar.set_postfix(\n",
        "                Loss=f\"{total_loss.item():.4f}\",\n",
        "                MSE=f\"{mse.item():.4f}\",\n",
        "                SSIM=f\"{ssim_score.item():.4f}\"\n",
        "            )\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        val_psnr, val_ssim = 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for i, (lr_images, hr_images) in enumerate(val_loader):\n",
        "                lr_images = lr_images.to(DEVICE)\n",
        "                hr_images = hr_images.to(DEVICE)\n",
        "                output = model(lr_images)\n",
        "                mse_val = nn.functional.mse_loss(output, hr_images)\n",
        "                val_psnr += psnr(mse_val.item())\n",
        "                val_ssim += ssim(output, hr_images, data_range=1.0, size_average=True).item()\n",
        "\n",
        "                if i == 0 and (epoch + 1) % 5 == 0:\n",
        "                    sr_pil = ToPILImage()(output.squeeze(0).cpu())\n",
        "                    output_path = OUTPUTS_DIR / f'val_epoch_{epoch+1}.png'\n",
        "                    sr_pil.save(output_path)\n",
        "                    print(f\"\\nSaved validation sample to {output_path}\")\n",
        "\n",
        "        avg_val_psnr = val_psnr / len(val_loader)\n",
        "        avg_val_ssim = val_ssim / len(val_loader)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "        print(f\"  - Validation PSNR: {avg_val_psnr:.4f} dB | Validation SSIM: {avg_val_ssim:.4f}\")\n",
        "        print(f\"  - Current Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "        training_log.append({\n",
        "            'epoch': epoch + 1,\n",
        "            'val_psnr': avg_val_psnr,\n",
        "            'val_ssim': avg_val_ssim,\n",
        "            'learning_rate': scheduler.get_last_lr()[0]\n",
        "        })\n",
        "\n",
        "        log_df = pd.DataFrame(training_log)\n",
        "        log_df.to_csv(log_file_path, index=False)\n",
        "        scheduler.step()\n",
        "\n",
        "        if avg_val_psnr > best_psnr:\n",
        "            best_psnr = avg_val_psnr\n",
        "            print(f\"  - New best model found! PSNR: {best_psnr:.4f} dB. Saving checkpoint...\")\n",
        "\n",
        "            # --- Check if model was compiled and strip prefix BEFORE saving ---\n",
        "            model_state = model.state_dict()\n",
        "            needs_stripping = any(key.startswith('_orig_mod.') for key in model_state.keys())\n",
        "\n",
        "            if needs_stripping:\n",
        "                print(\"  - (Saving) Stripping '_orig_mod.' prefix from compiled model...\")\n",
        "                final_model_state = {}\n",
        "                for key, value in model_state.items():\n",
        "                    # Remove the '_orig_mod.' prefix\n",
        "                    new_key = key[len('_orig_mod.'):] if key.startswith('_orig_mod.') else key\n",
        "                    final_model_state[new_key] = value\n",
        "            else:\n",
        "                final_model_state = model_state\n",
        "            # --- End of new block ---\n",
        "\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': final_model_state, # <-- Save the clean state\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_psnr': best_psnr,\n",
        "            }, CHECKPOINT_FILE)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nTraining complete in {total_time/3600:.2f} hours.\")\n",
        "    print(f\"Best model saved to '{CHECKPOINT_FILE}' with a PSNR of {best_psnr:.4f} dB.\")\n",
        "    print(f\"\\nTraining log saved successfully to {log_file_path}\")\n",
        "\n",
        "    print(\"Generating training graphs...\")\n",
        "\n",
        "    # Plot PSNR vs. Epoch\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(log_df['epoch'], log_df['val_psnr'], marker='o', color='b')\n",
        "    plt.title('Validation PSNR vs. Epoch')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('PSNR (dB)')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Plot SSIM vs. Epoch\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(log_df['epoch'], log_df['val_ssim'], marker='o', color='g')\n",
        "    plt.title('Validation SSIM vs. Epoch')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('SSIM')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1KSQx28dzQI",
      "metadata": {
        "id": "e1KSQx28dzQI"
      },
      "source": [
        "Convert To FP16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OT6qOk-Ad2xu",
      "metadata": {
        "id": "OT6qOk-Ad2xu"
      },
      "outputs": [],
      "source": [
        "print(\"--- Applying FP16 Quantization ---\")\n",
        "\n",
        "quant_model = ESPCN(upscale_factor=UPSCALE_FACTOR).to(DEVICE)\n",
        "checkpoint_path = CHECKPOINTS_DIR / f\"best_espcn_x{UPSCALE_FACTOR}.pth\"\n",
        "checkpoint = torch.load(CHECKPOINT_FILE, map_location=DEVICE)\n",
        "\n",
        "# <<<--- ADD THIS BLOCK --- >>>\n",
        "original_state_dict = checkpoint['model_state_dict']\n",
        "new_state_dict = {}\n",
        "needs_prefix_stripping = any(key.startswith('_orig_mod.') for key in original_state_dict.keys())\n",
        "if needs_prefix_stripping:\n",
        "    print(\"Detected '_orig_mod.' prefix from torch.compile(). Stripping prefix for FP16 conversion...\")\n",
        "    for key, value in original_state_dict.items():\n",
        "        if key.startswith('_orig_mod.'):\n",
        "            new_key = key[len('_orig_mod.'):] # Remove the prefix\n",
        "            new_state_dict[new_key] = value\n",
        "        else:\n",
        "            new_state_dict[key] = value # Keep keys without the prefix as is\n",
        "else:\n",
        "     print(\"No '_orig_mod.' prefix detected. Loading state dict as is for FP16 conversion.\")\n",
        "     new_state_dict = original_state_dict # Use the original if no prefix found\n",
        "\n",
        "quant_model.load_state_dict(new_state_dict) # <<<--- LOAD THE STRIPPED DICT\n",
        "# <<<--- END OF ADDED BLOCK --- >>>\n",
        "\n",
        "quant_model.eval()\n",
        "# Note: .half() is usually for GPU. On CPU, it might be slow or unsupported.\n",
        "# Consider keeping it FP32 for CPU-only export unless you have specific needs.\n",
        "# If you keep .half(), ensure subsequent ONNX export/benchmark handles FP16.\n",
        "quant_model.half()\n",
        "\n",
        "print(\"Model converted to FP16.\")\n",
        "\n",
        "quantized_model_path = CHECKPOINTS_DIR / f\"best_espcn_x{UPSCALE_FACTOR}_fp16.pth\"\n",
        "# Saving the state_dict directly is correct here\n",
        "torch.save(quant_model.state_dict(), quantized_model_path)\n",
        "\n",
        "print(f\"FP16 quantized model saved to: {quantized_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nT7gxmzLgwrN",
      "metadata": {
        "id": "nT7gxmzLgwrN"
      },
      "source": [
        "Export To ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vXI9Iz4XgyAN",
      "metadata": {
        "id": "vXI9Iz4XgyAN"
      },
      "outputs": [],
      "source": [
        "print(\"--- Exporting to ONNX ---\")\n",
        "\n",
        "EXPORT_FP16 = True\n",
        "\n",
        "if EXPORT_FP16:\n",
        "    print(\"Exporting FP16 model...\")\n",
        "    onnx_model = ESPCN(upscale_factor=UPSCALE_FACTOR)\n",
        "    checkpoint_path_fp16 = CHECKPOINTS_DIR / f\"best_espcn_x{UPSCALE_FACTOR}_fp16.pth\"\n",
        "    onnx_model.load_state_dict(torch.load(checkpoint_path_fp16))\n",
        "    onnx_model.eval().half().to(DEVICE)\n",
        "    dummy_input = torch.randn(1, 3, 720, 1280, device=DEVICE).half()\n",
        "    onnx_model_path = OUTPUTS_DIR / f\"espcn_x{UPSCALE_FACTOR}_fp16.onnx\"\n",
        "else:\n",
        "    print(\"Exporting FP32 model...\")\n",
        "    onnx_model = ESPCN(upscale_factor=UPSCALE_FACTOR).to(DEVICE)\n",
        "    checkpoint_path = CHECKPOINTS_DIR / f\"best_espcn_x{UPSCALE_FACTOR}.pth\"\n",
        "    checkpoint = torch.load(CHECKPOINT_FILE, map_location=DEVICE)\n",
        "\n",
        "    # <<< --- START: ADD THIS FIX --- >>>\n",
        "    original_state_dict = checkpoint['model_state_dict']\n",
        "    new_state_dict = {}\n",
        "    needs_prefix_stripping = any(key.startswith('_orig_mod.') for key in original_state_dict.keys())\n",
        "\n",
        "    if needs_prefix_stripping:\n",
        "        print(\"Detected '_orig_mod.' prefix. Stripping for ONNX export...\")\n",
        "        for key, value in original_state_dict.items():\n",
        "            if key.startswith('_orig_mod.'):\n",
        "                new_key = key[len('_orig_mod.'):] # Remove the prefix\n",
        "                new_state_dict[new_key] = value\n",
        "            else:\n",
        "                new_state_dict[key] = value\n",
        "    else:\n",
        "         print(\"No '_orig_mod.' prefix detected. Loading state dict as is for ONNX export.\")\n",
        "         new_state_dict = original_state_dict\n",
        "\n",
        "    onnx_model.load_state_dict(new_state_dict) # <<< --- LOAD THE STRIPPED DICT\n",
        "    # <<< --- END: ADD THIS FIX --- >>>\n",
        "\n",
        "    onnx_model.eval()\n",
        "    dummy_input = torch.randn(1, 3, 720, 1280, device=DEVICE)\n",
        "    onnx_model_path = OUTPUTS_DIR / f\"espcn_x{UPSCALE_FACTOR}.onnx\"\n",
        "\n",
        "torch.onnx.export(\n",
        "    onnx_model,\n",
        "    dummy_input,\n",
        "    str(onnx_model_path),\n",
        "    export_params=True,\n",
        "    do_constant_folding=True,\n",
        "    input_names=['input'],\n",
        "    output_names=['output'],\n",
        "    dynamo=True,\n",
        "    dynamic_axes={\n",
        "        'input': {2: 'height', 3: 'width'},\n",
        "        'output': {2: 'height_out', 3: 'width_out'}\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Model successfully exported to ONNX: {onnx_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jnf5DCUhhzop",
      "metadata": {
        "id": "jnf5DCUhhzop"
      },
      "source": [
        "Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vaqAOnrmh1Fo",
      "metadata": {
        "id": "vaqAOnrmh1Fo"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Running Inference Benchmark ---\")\n",
        "\n",
        "if EXPORT_FP16:\n",
        "    test_image_np = np.random.randn(1, 3, 720, 1280).astype(np.float16)\n",
        "else:\n",
        "    test_image_np = np.random.randn(1, 3, 720, 1280).astype(np.float32)\n",
        "test_image_torch = torch.from_numpy(test_image_np).to(DEVICE)\n",
        "\n",
        "if EXPORT_FP16:\n",
        "    test_image_torch = test_image_torch.half()\n",
        "\n",
        "if DEVICE == \"cuda\":\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "with torch.no_grad():\n",
        "    _ = onnx_model(test_image_torch)\n",
        "    if DEVICE == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    start_time = time.time()\n",
        "    for _ in range(100):\n",
        "        _ = onnx_model(test_image_torch)\n",
        "    if DEVICE == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    pytorch_time = (time.time() - start_time) / 100\n",
        "\n",
        "print(f\"PyTorch Inference Time: {pytorch_time * 1000:.4f} ms per image\")\n",
        "\n",
        "providers = [\n",
        "    # ==> Tier 1: Highest-Performance, Hardware-Specific Providers (NVIDIA, AMD, Intel, Apple)\n",
        "    'TensorrtExecutionProvider',      # NVIDIA's top-tier for speed\n",
        "    'CUDAExecutionProvider',          # Standard NVIDIA GPU provider\n",
        "    'MIGraphXExecutionProvider',      # AMD's high-performance graph compiler on Linux\n",
        "    'ROCmExecutionProvider',          # Standard AMD GPU provider on Linux\n",
        "    'OpenVINOExecutionProvider',      # Optimized for Intel GPUs and CPUs\n",
        "    'CoreMLExecutionProvider',        # For Apple M-series chips (macOS, iOS)\n",
        "\n",
        "    # ==> Tier 2: Specialized NPU/Edge/Mobile Providers\n",
        "    'QNNExecutionProvider',           # Qualcomm AI Engine Direct (Snapdragon)\n",
        "    'NNAPIExecutionProvider',         # Android Neural Networks API\n",
        "    'CANNExecutionProvider',          # Huawei Ascend Chips\n",
        "    'RockchipNpuExecutionProvider',   # Rockchip NPUs\n",
        "    'VitisAIExecutionProvider',       # Xilinx FPGAs\n",
        "    'ArmNNExecutionProvider',         # Arm NN SDK\n",
        "    'ACLExecutionProvider',           # Arm Compute Library\n",
        "\n",
        "    # ==> Tier 3: General-Purpose GPU Provider (Windows)\n",
        "    'DmlExecutionProvider',           # DirectX 12 for NVIDIA, AMD, Intel GPUs on Windows\n",
        "\n",
        "    # ==> Tier 4: Optimized CPU Providers\n",
        "    'DnnlExecutionProvider',          # Intel's high-performance DNNL for CPUs\n",
        "    'XnnpackExecutionProvider',       # Optimized for ARM CPUs\n",
        "\n",
        "    # ==> Tier 5: Advanced Compiler & Cloud Providers\n",
        "    'TvmExecutionProvider',           # Apache TVM\n",
        "    'AzureExecutionProvider',         # For running on Azure\n",
        "\n",
        "    # ==> Tier 6: Default Fallback\n",
        "    'CPUExecutionProvider',           # The universal fallback that runs on any CPU\n",
        "]\n",
        "\n",
        "session = ort.InferenceSession(str(onnx_model_path), providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
        "ort_inputs = {session.get_inputs()[0].name: test_image_np}\n",
        "_ = session.run(None, ort_inputs)\n",
        "start_time = time.time()\n",
        "for _ in range(100):\n",
        "    _ = session.run(None, ort_inputs)\n",
        "onnx_time = (time.time() - start_time) / 100\n",
        "\n",
        "print(f\"ONNX Runtime Inference Time: {onnx_time * 1000:.4f} ms per image\")\n",
        "\n",
        "if pytorch_time > 0:\n",
        "    speed_increase = (pytorch_time - onnx_time) / pytorch_time * 100\n",
        "\n",
        "    print(f\"\\nResult: ONNX Runtime is ~{speed_increase:.2f}% faster than PyTorch for this model.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
