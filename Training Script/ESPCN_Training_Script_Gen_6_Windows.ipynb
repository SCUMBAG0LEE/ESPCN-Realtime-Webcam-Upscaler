{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb08e3af",
   "metadata": {},
   "source": [
    "# ESPCN Realtime Webcam Upscaler - Training Script (Windows)\n",
    "\n",
    "**Project:** ESPCN Realtime Webcam Upscaler  \n",
    "**Institution:** Universitas Tarumanagara  \n",
    "**Thesis:** Development Of An ESPCN Deep Learning Model For Real-Time Webcam Video Super-Resolution With Hardware Interference Optimization  \n",
    "**Platform:** Windows Local Machine  \n",
    "**Date:** January 2026\n",
    "\n",
    "## Environment Specification\n",
    "- **OS:** Windows 11\n",
    "- **Python:** 3.12\n",
    "- **CPU:** Intel Core I7-7700 (4 Core / 8 Thread)\n",
    "- **RAM:** 64GB\n",
    "- **GPU:** Nvidia P104-100 (8GB VRAM)\n",
    "- **Framework:** PyTorch 2.x with CUDA support\n",
    "\n",
    "## Overview\n",
    "This notebook implements training of the ESPCN (Efficient Sub-Pixel Convolutional Neural Network) model for 2x super-resolution of webcam video streams. The training pipeline includes:\n",
    "- Dataset preparation with caching for efficiency\n",
    "- Combined MSE + SSIM loss function\n",
    "- Mixed precision training with automatic casting\n",
    "- Model checkpointing with resume capability\n",
    "- FP16 quantization and ONNX export\n",
    "- Performance benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29cbc29",
   "metadata": {
    "id": "e29cbc29"
   },
   "source": [
    "## 1. Library Imports\n",
    "\n",
    "Import all required libraries for training, data processing, and model optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2675fdef",
   "metadata": {
    "id": "2675fdef"
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision.transforms import ToTensor, ToPILImage, Compose, ColorJitter\n",
    "\n",
    "# Progress bar and utilities\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ONNX and inference\n",
    "import onnxruntime as ort\n",
    "from pytorch_msssim import ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09162a59",
   "metadata": {
    "id": "09162a59"
   },
   "source": [
    "## 2. Training Configuration & Hyperparameters\n",
    "\n",
    "Configure model architecture, training schedule, and optimization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e96d0",
   "metadata": {
    "id": "731e96d0"
   },
   "outputs": [],
   "source": [
    "# ========================= CONFIGURATION =========================\n",
    "# Model Architecture\n",
    "UPSCALE_FACTOR = 2  # 2x upscaling (e.g., 360p â†’ 720p)\n",
    "\n",
    "# Training Hyperparameters\n",
    "BATCH_SIZE = 64  # Batch size for training (adjust based on GPU memory)\n",
    "EPOCHS = 150  # Total training epochs\n",
    "PATCH_SIZE = 256  # Size of training patches extracted from HR images\n",
    "LEARNING_RATE = 1e-4  # Adam optimizer learning rate\n",
    "LOSS_ALPHA = 0.84  # Weight for MSE loss in combined loss (SSIM weight = 1 - LOSS_ALPHA)\n",
    "\n",
    "# Hardware & Device\n",
    "NUM_WORKER = 0  # Number of DataLoader workers (0 = no multiprocessing, set based on CPU cores)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Directory Structure\n",
    "BASE_DIR = Path(\".\")\n",
    "DATA_DIR = BASE_DIR / \"datasets\"\n",
    "OUTPUTS_DIR = BASE_DIR / \"outputs\"\n",
    "CHECKPOINTS_DIR = BASE_DIR / \"checkpoints\"\n",
    "CHECKPOINT_FILE = CHECKPOINTS_DIR / f\"best_espcn_x{UPSCALE_FACTOR}.pth\"\n",
    "\n",
    "print(f\"Configuration loaded. Device: {DEVICE} | Batch Size: {BATCH_SIZE} | Loss Alpha: {LOSS_ALPHA}\")\n",
    "\n",
    "# ========================= REPRODUCIBILITY =========================\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility across NumPy, PyTorch, and CUDA.\n",
    "    \n",
    "    Args:\n",
    "        seed (int): Random seed value\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Ensure deterministic behavior (may impact performance)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    \"\"\"\n",
    "    Set seed for DataLoader worker processes.\n",
    "    \n",
    "    Args:\n",
    "        worker_id (int): Worker process ID\n",
    "    \"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806b70ea",
   "metadata": {
    "id": "806b70ea"
   },
   "source": [
    "## 3. Dataset Preparation & Environment Setup\n",
    "\n",
    "Prepare datasets from DIV2K, Flickr2K, and optional personal datasets. Uses caching for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4d01f6",
   "metadata": {
    "id": "4d4d01f6"
   },
   "outputs": [],
   "source": [
    "def prepare_environment_and_datasets(patch_size):\n",
    "    \"\"\"\n",
    "    Prepare training environment and validate dataset structure.\n",
    "    \n",
    "    Creates necessary directories and builds a cached list of valid training/validation images.\n",
    "    Only images larger than or equal to patch_size are included in the training set.\n",
    "    \n",
    "    Args:\n",
    "        patch_size (int): Minimum image dimension required for training patches\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (training_image_paths, validation_image_paths) - lists of valid image file paths\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: If no training or validation images are found\n",
    "    \"\"\"\n",
    "    # Create output directories\n",
    "    CHECKPOINTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    OUTPUTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    CACHE_FILE_TRAIN = OUTPUTS_DIR / \"valid_train_paths.txt\"\n",
    "\n",
    "    # ========== Validation Dataset ==========\n",
    "    # Validation paths are scanned fresh each run (fast operation)\n",
    "    valid_div2k_dir = DATA_DIR / \"DIV2K_valid_HR\"\n",
    "    validation_image_paths = glob.glob(str(valid_div2k_dir / '*.*'))\n",
    "\n",
    "    # ========== Training Dataset (with Caching) ==========\n",
    "    # Check if cache exists to avoid re-scanning large training directories\n",
    "    valid_train_paths = []\n",
    "    if CACHE_FILE_TRAIN.exists():\n",
    "        print(f\"Loading cached training image paths from {CACHE_FILE_TRAIN}...\")\n",
    "        try:\n",
    "            with open(CACHE_FILE_TRAIN, 'r') as f:\n",
    "                valid_train_paths = [line.strip() for line in f if line.strip()]\n",
    "            print(f\"âœ“ Loaded {len(valid_train_paths)} paths from cache.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš  Warning: Could not read cache file: {e}. Rebuilding...\")\n",
    "            valid_train_paths = []\n",
    "\n",
    "    # If cache miss or invalid, rebuild the training path list\n",
    "    if not valid_train_paths:\n",
    "        print(f\"Building new training image list from datasets...\")\n",
    "        train_div2k_dir = DATA_DIR / \"DIV2K_train_HR\"\n",
    "        train_flickr2k_dir = DATA_DIR / \"Flickr2K_HR\"\n",
    "        personal_dir = DATA_DIR / \"Personal_HR\"\n",
    "\n",
    "        # Combine DIV2K and Flickr2K datasets\n",
    "        train_image_paths = glob.glob(str(train_div2k_dir / '*.*')) + \\\n",
    "                            glob.glob(str(train_flickr2k_dir / '*.*'))\n",
    "\n",
    "        # Optionally add custom personal images\n",
    "        if personal_dir.exists():\n",
    "            print(\"âœ“ Personal dataset found. Adding to training set.\")\n",
    "            train_image_paths += glob.glob(str(personal_dir / '*.*'))\n",
    "        else:\n",
    "            print(\"â„¹ Personal dataset not found. Proceeding without it.\")\n",
    "\n",
    "        print(f\"Found {len(train_image_paths)} candidate training images.\")\n",
    "        print(f\"Validating images (minimum dimension: {patch_size}px)...\")\n",
    "\n",
    "        def is_image_large_enough(image_path, min_size):\n",
    "            \"\"\"Check if image meets minimum size requirements.\"\"\"\n",
    "            try:\n",
    "                img = cv2.imread(str(image_path))\n",
    "                if img is None:\n",
    "                    return False\n",
    "                h, w = img.shape[:2]\n",
    "                return h >= min_size and w >= min_size\n",
    "            except Exception as e:\n",
    "                print(f\"  âš  Error reading {Path(image_path).name}: {e}\")\n",
    "                return False\n",
    "\n",
    "        valid_train_paths = [\n",
    "            p for p in tqdm(train_image_paths, desc=\"Filtering images\")\n",
    "            if is_image_large_enough(p, patch_size)\n",
    "        ]\n",
    "\n",
    "        print(f\"âœ“ Filtered training set: {len(valid_train_paths)}/{len(train_image_paths)} images retained.\")\n",
    "\n",
    "        # Save cache for future runs\n",
    "        try:\n",
    "            with open(CACHE_FILE_TRAIN, 'w') as f:\n",
    "                for path in valid_train_paths:\n",
    "                    f.write(f\"{path}\\n\")\n",
    "            print(f\"âœ“ Cached training paths to {CACHE_FILE_TRAIN}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš  Warning: Could not write cache file: {e}\")\n",
    "\n",
    "    # Verify datasets are not empty\n",
    "    if not valid_train_paths or not validation_image_paths:\n",
    "        raise RuntimeError(\n",
    "            \"\\nâœ— DATASET ERROR: No training or validation images found.\\n\"\n",
    "            f\"  Check that the following directories exist and contain images:\\n\"\n",
    "            f\"  - Training: {DATA_DIR / 'DIV2K_train_HR'}\\n\"\n",
    "            f\"  - Validation: {DATA_DIR / 'DIV2K_valid_HR'}\"\n",
    "        )\n",
    "\n",
    "    return valid_train_paths, validation_image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a1cbcd",
   "metadata": {
    "id": "a0a1cbcd"
   },
   "source": [
    "## 4. Loss Function Definition\n",
    "\n",
    "Defines combined MSE + SSIM loss function for training. This hybrid loss improves both pixel-level accuracy and perceptual quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da2e21",
   "metadata": {
    "id": "85da2e21"
   },
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined MSE and SSIM loss function for super-resolution training.\n",
    "    \n",
    "    Combines pixel-level Mean Squared Error (MSE) loss with perceptual \n",
    "    Structural Similarity Index (SSIM) loss for improved visual quality.\n",
    "    \n",
    "    Formula:\n",
    "        Loss = Î± * MSE + (1 - Î±) * (1 - SSIM)\n",
    "    \n",
    "    Args:\n",
    "        alpha (float): Weight for MSE loss component. Default: 0.7\n",
    "                      Set to 0.84 for better perceptual quality in thesis implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.7):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        try:\n",
    "            self.ssim_loss_fn = ssim\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"pytorch-msssim not found. Install with:\\n\"\n",
    "                \"  pip install pytorch-msssim\"\n",
    "            )\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        \"\"\"\n",
    "        Calculate combined loss.\n",
    "        \n",
    "        Args:\n",
    "            output (torch.Tensor): Model output, shape (B, C, H, W)\n",
    "            target (torch.Tensor): Ground truth, shape (B, C, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (total_loss, mse_loss, ssim_score)\n",
    "        \"\"\"\n",
    "        # MSE Loss - Pixel-level accuracy\n",
    "        loss_mse = self.mse_loss(output, target)\n",
    "\n",
    "        # SSIM Loss - Perceptual similarity (1 - SSIM, where 1 is worst, 0 is best)\n",
    "        ssim_score = self.ssim_loss_fn(output, target, data_range=1.0, size_average=True)\n",
    "        loss_ssim = 1 - ssim_score\n",
    "\n",
    "        # Combined Loss\n",
    "        total_loss = self.alpha * loss_mse + (1 - self.alpha) * loss_ssim\n",
    "\n",
    "        return total_loss, loss_mse, ssim_score\n",
    "\n",
    "\n",
    "def psnr(mse):\n",
    "    \"\"\"\n",
    "    Calculate Peak Signal-to-Noise Ratio (PSNR) from Mean Squared Error.\n",
    "    \n",
    "    Formula:\n",
    "        PSNR = 10 * log10(1 / MSE) dB\n",
    "    \n",
    "    Args:\n",
    "        mse (float): Mean Squared Error value\n",
    "        \n",
    "    Returns:\n",
    "        float: PSNR in decibels (dB)\n",
    "    \"\"\"\n",
    "    return 10 * math.log10(1 / mse) if mse > 0 else float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6698a64d",
   "metadata": {
    "id": "6698a64d"
   },
   "source": [
    "## 5. Dataset Classes\n",
    "\n",
    "PyTorch Dataset classes for training and validation with automatic patch creation and augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f942143",
   "metadata": {
    "id": "2f942143"
   },
   "outputs": [],
   "source": [
    "class TrainingSuperResolutionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Training dataset for super-resolution with online augmentation.\n",
    "    \n",
    "    Generates random patches from high-resolution images, applies geometric and\n",
    "    color augmentations, then creates corresponding low-resolution patches via\n",
    "    bicubic downsampling.\n",
    "    \n",
    "    Args:\n",
    "        image_filenames (list): Paths to high-resolution training images\n",
    "        crop_size (int): Size of patches to extract (will be adjusted to multiple of upscale_factor)\n",
    "        upscale_factor (int): Upscaling factor (2 for 2x, 4 for 4x, etc.)\n",
    "    \"\"\"\n",
    "    def __init__(self, image_filenames, crop_size, upscale_factor):\n",
    "        super(TrainingSuperResolutionDataset, self).__init__()\n",
    "        self.image_filenames = image_filenames\n",
    "        # Ensure patch size is divisible by upscale factor\n",
    "        self.crop_size = crop_size - (crop_size % upscale_factor)\n",
    "        self.upscale_factor = upscale_factor\n",
    "        self.to_tensor = ToTensor()\n",
    "        self.scale_factor = 1 / upscale_factor\n",
    "        \n",
    "        # Data augmentation pipeline\n",
    "        self.transform = Compose([\n",
    "            ToPILImage(),\n",
    "            ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get training sample at given index.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (low_res_patch, high_res_patch) - both torch.Tensor of shape (3, H, W)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load HR image\n",
    "            hr_image = cv2.imread(self.image_filenames[index])\n",
    "            if hr_image is None:\n",
    "                raise IOError(f\"Failed to load: {self.image_filenames[index]}\")\n",
    "            hr_image = cv2.cvtColor(hr_image, cv2.COLOR_BGR2RGB)\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Error loading {self.image_filenames[index]}: {e}\")\n",
    "            # Fallback to next image\n",
    "            return self.__getitem__((index + 1) % len(self))\n",
    "\n",
    "        h, w = hr_image.shape[:2]\n",
    "\n",
    "        # Random crop\n",
    "        i = random.randint(0, h - self.crop_size)\n",
    "        j = random.randint(0, w - self.crop_size)\n",
    "        hr_patch = hr_image[i:i+self.crop_size, j:j+self.crop_size, :]\n",
    "\n",
    "        # Random horizontal flip\n",
    "        if torch.rand(1) > 0.5:\n",
    "            hr_patch = cv2.flip(hr_patch, 1)\n",
    "\n",
    "        # Random 90Â° rotation\n",
    "        if torch.rand(1) > 0.5:\n",
    "            angle = random.choice([\n",
    "                cv2.ROTATE_90_CLOCKWISE,\n",
    "                cv2.ROTATE_180,\n",
    "                cv2.ROTATE_90_COUNTERCLOCKWISE\n",
    "            ])\n",
    "            hr_patch = cv2.rotate(hr_patch, angle)\n",
    "\n",
    "        # Apply color augmentation and convert to tensor\n",
    "        hr_patch_tensor = self.transform(hr_patch)\n",
    "\n",
    "        # Create LR from augmented HR via bicubic downsampling\n",
    "        lr_patch_tensor = F.interpolate(\n",
    "            hr_patch_tensor.unsqueeze(0),\n",
    "            scale_factor=self.scale_factor,\n",
    "            mode='bicubic',\n",
    "            align_corners=False,\n",
    "            antialias=True\n",
    "        ).squeeze(0)\n",
    "\n",
    "        return lr_patch_tensor, hr_patch_tensor\n",
    "\n",
    "\n",
    "class ValidationSuperResolutionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Validation dataset for super-resolution (no augmentation).\n",
    "    \n",
    "    Loads full validation images and creates corresponding low-resolution\n",
    "    versions via bicubic downsampling for evaluation.\n",
    "    \n",
    "    Args:\n",
    "        image_filenames (list): Paths to high-resolution validation images\n",
    "        upscale_factor (int): Upscaling factor (2 for 2x, 4 for 4x, etc.)\n",
    "    \"\"\"\n",
    "    def __init__(self, image_filenames, upscale_factor):\n",
    "        super(ValidationSuperResolutionDataset, self).__init__()\n",
    "        self.image_filenames = image_filenames\n",
    "        self.upscale_factor = upscale_factor\n",
    "        self.to_tensor = ToTensor()\n",
    "        self.scale_factor = 1 / upscale_factor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get validation sample at given index.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (low_res_image, high_res_image) - both torch.Tensor of shape (3, H, W)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load HR image\n",
    "            hr_image = cv2.imread(self.image_filenames[index])\n",
    "            if hr_image is None:\n",
    "                raise IOError(f\"Failed to load: {self.image_filenames[index]}\")\n",
    "            hr_image = cv2.cvtColor(hr_image, cv2.COLOR_BGR2RGB)\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Error loading {self.image_filenames[index]}: {e}\")\n",
    "            return self.__getitem__((index + 1) % len(self))\n",
    "\n",
    "        h, w = hr_image.shape[:2]\n",
    "        # Crop to multiple of upscale_factor to avoid artifacts\n",
    "        w_new = w - (w % self.upscale_factor)\n",
    "        h_new = h - (h % self.upscale_factor)\n",
    "        hr_image = hr_image[:h_new, :w_new, :]\n",
    "\n",
    "        # Convert to tensor\n",
    "        hr_tensor = self.to_tensor(hr_image)\n",
    "\n",
    "        # Downsample to LR via bicubic interpolation\n",
    "        lr_tensor = F.interpolate(\n",
    "            hr_tensor.unsqueeze(0),\n",
    "            scale_factor=self.scale_factor,\n",
    "            mode='bicubic',\n",
    "            align_corners=False,\n",
    "            antialias=True\n",
    "        ).squeeze(0)\n",
    "\n",
    "        return lr_tensor, hr_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e70967",
   "metadata": {
    "id": "a9e70967"
   },
   "source": [
    "## 6. Model Architecture\n",
    "\n",
    "Efficient Sub-Pixel Convolutional Neural Network (ESPCN) model for super-resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ccae0",
   "metadata": {
    "id": "3e5ccae0"
   },
   "outputs": [],
   "source": [
    "class ESPCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Efficient Sub-Pixel Convolutional Neural Network (ESPCN).\n",
    "    \n",
    "    A lightweight network for super-resolution using sub-pixel convolution.\n",
    "    Learns upsampling in the feature space, then uses pixel shuffle for efficient\n",
    "    spatial upsampling.\n",
    "    \n",
    "    Architecture:\n",
    "        - Conv2d(3 â†’ 64, 5Ã—5) + ReLU\n",
    "        - Conv2d(64 â†’ 64, 3Ã—3) + ReLU\n",
    "        - Conv2d(64 â†’ 3*rÂ², 3Ã—3) where r = upscale_factor\n",
    "        - PixelShuffle(r) â†’ output (3, H*r, W*r)\n",
    "    \n",
    "    Args:\n",
    "        upscale_factor (int): Upscaling factor (2 for 2x, 4 for 4x, etc.)\n",
    "    \"\"\"\n",
    "    def __init__(self, upscale_factor):\n",
    "        super(ESPCN, self).__init__()\n",
    "        self.upscale_factor = upscale_factor\n",
    "        \n",
    "        # Feature extraction layer (5Ã—5 receptive field)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
    "        \n",
    "        # Non-linear mapping layer\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Sub-pixel convolution layer\n",
    "        # Output channels = 3 (RGB) * (upscale_factorÂ²) for reshuffling\n",
    "        self.conv3 = nn.Conv2d(64, 3 * (upscale_factor ** 2), kernel_size=3, padding=1)\n",
    "        \n",
    "        # Pixel shuffle for efficient spatial upsampling\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
    "        \n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, 3, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (B, 3, H*r, W*r) where r = upscale_factor\n",
    "        \"\"\"\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pixel_shuffle(self.conv3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d250a4ae",
   "metadata": {
    "id": "d250a4ae"
   },
   "source": [
    "## 7. Main Training Loop\n",
    "\n",
    "Execute the complete training pipeline with validation, checkpointing, and metric logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197dcee6",
   "metadata": {
    "id": "197dcee6"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ESPCN TRAINING INITIALIZATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Device Configuration:\")\n",
    "    print(f\"  Device: {DEVICE}\")\n",
    "    print(f\"  CUDA Available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  GPU Count: {torch.cuda.device_count()}\")\n",
    "        print(f\"  GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    # ========== Prepare Datasets ==========\n",
    "    print(f\"\\nðŸ“ Preparing datasets...\")\n",
    "    train_paths, val_paths = prepare_environment_and_datasets(PATCH_SIZE)\n",
    "\n",
    "    if not train_paths or not val_paths:\n",
    "        raise RuntimeError(\"âœ— Dataset preparation failed. Check your data directories.\")\n",
    "\n",
    "    # ========== Create PyTorch Datasets ==========\n",
    "    train_dataset = TrainingSuperResolutionDataset(\n",
    "        train_paths, crop_size=PATCH_SIZE, upscale_factor=UPSCALE_FACTOR\n",
    "    )\n",
    "    val_dataset = ValidationSuperResolutionDataset(\n",
    "        val_paths, upscale_factor=UPSCALE_FACTOR\n",
    "    )\n",
    "\n",
    "    # ========== Create DataLoaders ==========\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKER,\n",
    "        pin_memory=(DEVICE == \"cuda\"),\n",
    "        worker_init_fn=seed_worker if NUM_WORKER > 0 else None\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKER,\n",
    "        pin_memory=(DEVICE == \"cuda\")\n",
    "    )\n",
    "\n",
    "    print(f\"\\nâœ“ Datasets loaded successfully:\")\n",
    "    print(f\"  Training samples: {len(train_dataset)}\")\n",
    "    print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"  Training batches: {len(train_loader)}\")\n",
    "\n",
    "    # ========== Initialize Model ==========\n",
    "    model = ESPCN(upscale_factor=UPSCALE_FACTOR)\n",
    "    if DEVICE == 'cuda' and torch.cuda.device_count() > 1:\n",
    "        print(f\"\\nðŸ”— Using DataParallel with {torch.cuda.device_count()} GPUs\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # Count parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nðŸ§  Model: ESPCN x{UPSCALE_FACTOR} ({num_params:,} parameters)\")\n",
    "\n",
    "    # ========== Initialize Training Components ==========\n",
    "    criterion = CombinedLoss(alpha=LOSS_ALPHA).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "    scaler = GradScaler(enabled=(DEVICE == \"cuda\"))\n",
    "\n",
    "    # ========== Checkpoint & Logging Setup ==========\n",
    "    start_epoch = 0\n",
    "    best_psnr = 0.0\n",
    "    log_file_path = OUTPUTS_DIR / 'training_log.csv'\n",
    "    training_log = []\n",
    "\n",
    "    if CHECKPOINT_FILE.exists():\n",
    "        print(f\"\\nâ†» Loading checkpoint: {CHECKPOINT_FILE}\")\n",
    "        checkpoint = torch.load(CHECKPOINT_FILE, map_location=DEVICE)\n",
    "        \n",
    "        # Handle state dict compatibility (torch.compile, DataParallel prefixes)\n",
    "        original_state_dict = checkpoint['model_state_dict']\n",
    "        new_state_dict = {}\n",
    "\n",
    "        # Strip '_orig_mod.' prefix from torch.compile\n",
    "        needs_prefix_stripping = any(key.startswith('_orig_mod.') for key in original_state_dict.keys())\n",
    "        if needs_prefix_stripping:\n",
    "            print(\"  - Detected '_orig_mod.' prefix (torch.compile). Stripping...\")\n",
    "            for key, value in original_state_dict.items():\n",
    "                new_key = key[len('_orig_mod.'):] if key.startswith('_orig_mod.') else key\n",
    "                new_state_dict[new_key] = value\n",
    "        else:\n",
    "            new_state_dict = original_state_dict\n",
    "\n",
    "        # Strip 'module.' prefix from DataParallel\n",
    "        needs_module_stripping = any(key.startswith('module.') for key in new_state_dict.keys())\n",
    "        if needs_module_stripping:\n",
    "            print(\"  - Detected 'module.' prefix (DataParallel). Stripping...\")\n",
    "            stripped_dict = {}\n",
    "            for key, value in new_state_dict.items():\n",
    "                new_key = key[len('module.'):] if key.startswith('module.') else key\n",
    "                stripped_dict[new_key] = value\n",
    "            new_state_dict = stripped_dict\n",
    "\n",
    "        model.load_state_dict(new_state_dict)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_psnr = checkpoint.get('best_psnr', 0.0)\n",
    "        \n",
    "        print(f\"  âœ“ Resumed from epoch {start_epoch} (best PSNR: {best_psnr:.4f} dB)\")\n",
    "\n",
    "        # Load previous training log\n",
    "        if log_file_path.exists():\n",
    "            try:\n",
    "                log_df = pd.read_csv(log_file_path)\n",
    "                log_df = log_df[log_df['epoch'] < start_epoch]\n",
    "                training_log = log_df.to_dict('records')\n",
    "                print(f\"  âœ“ Loaded {len(training_log)} previous log entries\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âš  Could not load log: {e}. Starting fresh.\")\n",
    "                training_log = []\n",
    "    else:\n",
    "        print(f\"\\nâ–¶ Starting training from scratch (no checkpoint found)\")\n",
    "        if log_file_path.exists():\n",
    "            log_file_path.unlink()\n",
    "            print(f\"  - Cleared old log file\")\n",
    "\n",
    "    # ========== BEGIN TRAINING ==========\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TRAINING ESPCN x{UPSCALE_FACTOR} | Epochs: {start_epoch+1} â†’ {EPOCHS}\")\n",
    "    print(f\"Loss: Combined MSE (Î±={LOSS_ALPHA}) + SSIM (Î±={1-LOSS_ALPHA})\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(start_epoch, EPOCHS):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", unit=\"batch\")\n",
    "        for batch_idx, (lr_images, hr_images) in enumerate(progress_bar):\n",
    "            lr_images = lr_images.to(DEVICE, non_blocking=(DEVICE == \"cuda\"))\n",
    "            hr_images = hr_images.to(DEVICE, non_blocking=(DEVICE == \"cuda\"))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with mixed precision for GPU\n",
    "            if DEVICE == \"cuda\":\n",
    "                with autocast(device_type=DEVICE):\n",
    "                    outputs = model(lr_images)\n",
    "                    total_loss, mse, ssim_score = criterion(outputs.float(), hr_images.float())\n",
    "                \n",
    "                # Backward pass with gradient scaling\n",
    "                scaler.scale(total_loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # CPU: standard forward/backward without mixed precision\n",
    "                outputs = model(lr_images)\n",
    "                total_loss, mse, ssim_score = criterion(outputs.float(), hr_images.float())\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            epoch_train_loss += total_loss.item()\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f\"{total_loss.item():.4f}\",\n",
    "                'MSE': f\"{mse.item():.6f}\",\n",
    "                'SSIM': f\"{ssim_score.item():.4f}\"\n",
    "            })\n",
    "\n",
    "        # ========== VALIDATION ==========\n",
    "        model.eval()\n",
    "        val_psnr, val_ssim = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, (lr_images, hr_images) in enumerate(val_loader):\n",
    "                lr_images = lr_images.to(DEVICE)\n",
    "                hr_images = hr_images.to(DEVICE)\n",
    "                \n",
    "                output = model(lr_images)\n",
    "                mse_val = nn.functional.mse_loss(output, hr_images)\n",
    "                val_psnr += psnr(mse_val.item())\n",
    "                val_ssim += ssim(output, hr_images, data_range=1.0, size_average=True).item()\n",
    "\n",
    "                # Save sample output every 5 epochs\n",
    "                if i == 0 and (epoch + 1) % 5 == 0:\n",
    "                    sr_pil = ToPILImage()(output.squeeze(0).cpu())\n",
    "                    output_path = OUTPUTS_DIR / f'val_epoch_{epoch+1:03d}.png'\n",
    "                    sr_pil.save(output_path)\n",
    "\n",
    "        avg_val_psnr = val_psnr / len(val_loader)\n",
    "        avg_val_ssim = val_ssim / len(val_loader)\n",
    "\n",
    "        # Log metrics\n",
    "        training_log.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'val_psnr': avg_val_psnr,\n",
    "            'val_ssim': avg_val_ssim,\n",
    "            'learning_rate': scheduler.get_last_lr()[0]\n",
    "        })\n",
    "\n",
    "        log_df = pd.DataFrame(training_log)\n",
    "        log_df.to_csv(log_file_path, index=False)\n",
    "\n",
    "        # Epoch summary\n",
    "        print(f\"Epoch {epoch+1:3d} â”‚ \"\n",
    "              f\"PSNR: {avg_val_psnr:7.4f} dB â”‚ \"\n",
    "              f\"SSIM: {avg_val_ssim:.6f} â”‚ \"\n",
    "              f\"LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # ========== CHECKPOINT BEST MODEL ==========\n",
    "        if avg_val_psnr > best_psnr:\n",
    "            best_psnr = avg_val_psnr\n",
    "            print(f\"  â†‘ New best PSNR! ({best_psnr:.4f} dB) Saving checkpoint...\")\n",
    "            \n",
    "            # Get clean state dict (handle compiled models)\n",
    "            model_state = model.state_dict()\n",
    "            needs_stripping = any(key.startswith('_orig_mod.') for key in model_state.keys())\n",
    "            \n",
    "            if needs_stripping:\n",
    "                final_model_state = {\n",
    "                    key[len('_orig_mod.'):] if key.startswith('_orig_mod.') else key: value\n",
    "                    for key, value in model_state.items()\n",
    "                }\n",
    "            else:\n",
    "                final_model_state = model_state\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': final_model_state,\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_psnr': best_psnr,\n",
    "                'config': {\n",
    "                    'upscale_factor': UPSCALE_FACTOR,\n",
    "                    'loss_alpha': LOSS_ALPHA,\n",
    "                    'learning_rate': LEARNING_RATE,\n",
    "                }\n",
    "            }, CHECKPOINT_FILE)\n",
    "\n",
    "    # ========== TRAINING COMPLETE ==========\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"âœ“ TRAINING COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total time: {total_time/3600:.2f} hours ({total_time/60:.1f} minutes)\")\n",
    "    print(f\"Best model: {CHECKPOINT_FILE}\")\n",
    "    print(f\"Best PSNR: {best_psnr:.4f} dB\")\n",
    "    print(f\"Training log: {log_file_path}\\n\")\n",
    "\n",
    "    # ========== PLOT RESULTS ==========\n",
    "    print(\"Generating training graphs...\")\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # PSNR plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(log_df['epoch'], log_df['val_psnr'], marker='o', linewidth=2, markersize=3, color='#2E86AB')\n",
    "    plt.title('Validation PSNR vs. Epoch', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Epoch', fontsize=11)\n",
    "    plt.ylabel('PSNR (dB)', fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # SSIM plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(log_df['epoch'], log_df['val_ssim'], marker='o', linewidth=2, markersize=3, color='#A23B72')\n",
    "    plt.title('Validation SSIM vs. Epoch', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Epoch', fontsize=11)\n",
    "    plt.ylabel('SSIM', fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUTS_DIR / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"âœ“ Curves saved to {OUTPUTS_DIR / 'training_curves.png'}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1KSQx28dzQI",
   "metadata": {
    "id": "e1KSQx28dzQI"
   },
   "source": [
    "## 8. FP16 Quantization\n",
    "\n",
    "Convert trained model to half-precision floating point for reduced memory and faster inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OT6qOk-Ad2xu",
   "metadata": {
    "id": "OT6qOk-Ad2xu"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FP16 QUANTIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "quant_model = ESPCN(upscale_factor=UPSCALE_FACTOR).to(DEVICE)\n",
    "checkpoint = torch.load(CHECKPOINT_FILE, map_location=DEVICE)\n",
    "\n",
    "# Load and clean state dict\n",
    "original_state_dict = checkpoint['model_state_dict']\n",
    "new_state_dict = {}\n",
    "needs_prefix_stripping = any(key.startswith('_orig_mod.') for key in original_state_dict.keys())\n",
    "\n",
    "if needs_prefix_stripping:\n",
    "    print(\"  - Stripping '_orig_mod.' prefix...\")\n",
    "    for key, value in original_state_dict.items():\n",
    "        new_key = key[len('_orig_mod.'):] if key.startswith('_orig_mod.') else key\n",
    "        new_state_dict[new_key] = value\n",
    "else:\n",
    "    new_state_dict = original_state_dict\n",
    "\n",
    "quant_model.load_state_dict(new_state_dict)\n",
    "quant_model.eval()\n",
    "quant_model.half()  # Convert to FP16\n",
    "\n",
    "quantized_model_path = CHECKPOINTS_DIR / f\"best_espcn_x{UPSCALE_FACTOR}_fp16.pth\"\n",
    "torch.save(quant_model.state_dict(), quantized_model_path)\n",
    "\n",
    "print(f\"âœ“ Model converted to FP16 and saved: {quantized_model_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nT7gxmzLgwrN",
   "metadata": {
    "id": "nT7gxmzLgwrN"
   },
   "source": [
    "## 9. ONNX Export\n",
    "\n",
    "Export trained model to ONNX format for cross-platform inference (Windows, Linux, Mobile, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vXI9Iz4XgyAN",
   "metadata": {
    "id": "vXI9Iz4XgyAN"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ONNX EXPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "EXPORT_FP16 = True\n",
    "DUMMY_INPUT_SIZE = (1, 3, 720, 1280)  # (batch, channels, height, width)\n",
    "\n",
    "if EXPORT_FP16:\n",
    "    print(f\"\\nExporting FP16 model...\")\n",
    "    onnx_model = ESPCN(upscale_factor=UPSCALE_FACTOR)\n",
    "    checkpoint_path_fp16 = CHECKPOINTS_DIR / f\"best_espcn_x{UPSCALE_FACTOR}_fp16.pth\"\n",
    "    onnx_model.load_state_dict(torch.load(checkpoint_path_fp16))\n",
    "    onnx_model.eval().half().to(DEVICE)\n",
    "    dummy_input = torch.randn(*DUMMY_INPUT_SIZE, device=DEVICE).half()\n",
    "    onnx_model_path = OUTPUTS_DIR / f\"espcn_x{UPSCALE_FACTOR}_fp16.onnx\"\n",
    "else:\n",
    "    print(f\"\\nExporting FP32 model...\")\n",
    "    onnx_model = ESPCN(upscale_factor=UPSCALE_FACTOR).to(DEVICE)\n",
    "    checkpoint = torch.load(CHECKPOINT_FILE, map_location=DEVICE)\n",
    "\n",
    "    # Clean state dict\n",
    "    original_state_dict = checkpoint['model_state_dict']\n",
    "    new_state_dict = {}\n",
    "    needs_prefix_stripping = any(key.startswith('_orig_mod.') for key in original_state_dict.keys())\n",
    "\n",
    "    if needs_prefix_stripping:\n",
    "        print(\"  - Stripping '_orig_mod.' prefix...\")\n",
    "        for key, value in original_state_dict.items():\n",
    "            new_key = key[len('_orig_mod.'):] if key.startswith('_orig_mod.') else key\n",
    "            new_state_dict[new_key] = value\n",
    "    else:\n",
    "        new_state_dict = original_state_dict\n",
    "\n",
    "    onnx_model.load_state_dict(new_state_dict)\n",
    "    onnx_model.eval()\n",
    "    dummy_input = torch.randn(*DUMMY_INPUT_SIZE, device=DEVICE)\n",
    "    onnx_model_path = OUTPUTS_DIR / f\"espcn_x{UPSCALE_FACTOR}.onnx\"\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(\n",
    "    onnx_model,\n",
    "    dummy_input,\n",
    "    str(onnx_model_path),\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamo=False,\n",
    "    dynamic_axes={\n",
    "        'input': {2: 'height', 3: 'width'},\n",
    "        'output': {2: 'height_out', 3: 'width_out'}\n",
    "    },\n",
    "    opset_version=17\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Model exported to ONNX: {onnx_model_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jnf5DCUhhzop",
   "metadata": {
    "id": "jnf5DCUhhzop"
   },
   "source": [
    "## 10. Performance Benchmarking\n",
    "\n",
    "Benchmark PyTorch vs ONNX Runtime inference performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vaqAOnrmh1Fo",
   "metadata": {
    "id": "vaqAOnrmh1Fo"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"INFERENCE BENCHMARKING\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Prepare test input\n",
    "if EXPORT_FP16:\n",
    "    test_image_np = np.random.randn(1, 3, 720, 1280).astype(np.float16)\n",
    "else:\n",
    "    test_image_np = np.random.randn(1, 3, 720, 1280).astype(np.float32)\n",
    "\n",
    "test_image_torch = torch.from_numpy(test_image_np).to(DEVICE)\n",
    "if EXPORT_FP16:\n",
    "    test_image_torch = test_image_torch.half()\n",
    "\n",
    "# ========== PyTorch Benchmark ==========\n",
    "print(\"Benchmarking PyTorch inference (100 iterations)...\")\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Warmup\n",
    "    _ = onnx_model(test_image_torch)\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Timed run\n",
    "    start_time = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = onnx_model(test_image_torch)\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    pytorch_time = (time.time() - start_time) / 100\n",
    "\n",
    "print(f\"âœ“ PyTorch inference: {pytorch_time * 1000:.4f} ms/image\\n\")\n",
    "\n",
    "# ========== ONNX Runtime Benchmark ==========\n",
    "print(\"Benchmarking ONNX Runtime inference (100 iterations)...\")\n",
    "session = ort.InferenceSession(\n",
    "    str(onnx_model_path),\n",
    "    providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    ")\n",
    "print(f\"  Providers: {session.get_providers()}\")\n",
    "\n",
    "ort_inputs = {session.get_inputs()[0].name: test_image_np}\n",
    "\n",
    "# Warmup\n",
    "_ = session.run(None, ort_inputs)\n",
    "\n",
    "# Timed run\n",
    "start_time = time.time()\n",
    "for _ in range(100):\n",
    "    _ = session.run(None, ort_inputs)\n",
    "onnx_time = (time.time() - start_time) / 100\n",
    "\n",
    "print(f\"âœ“ ONNX Runtime inference: {onnx_time * 1000:.4f} ms/image\\n\")\n",
    "\n",
    "# ========== Results ==========\n",
    "print(\"=\"*70)\n",
    "if pytorch_time > 0:\n",
    "    if onnx_time < pytorch_time:\n",
    "        speedup = (pytorch_time - onnx_time) / pytorch_time * 100\n",
    "        print(f\"âœ“ ONNX is ~{speedup:.1f}% faster than PyTorch\")\n",
    "    else:\n",
    "        slowdown = (onnx_time - pytorch_time) / pytorch_time * 100\n",
    "        print(f\"âš  ONNX is ~{slowdown:.1f}% slower than PyTorch\")\n",
    "else:\n",
    "    print(\"âš  Could not compute speedup (PyTorch time = 0)\")\n",
    "\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "14AB3UkpiuPOQecEDK3iAB5PhTfQLrrnd",
     "timestamp": 1760865764796
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
