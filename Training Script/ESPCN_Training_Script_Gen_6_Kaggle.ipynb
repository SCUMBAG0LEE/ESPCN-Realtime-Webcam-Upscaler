{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3dff22d",
   "metadata": {},
   "source": [
    "Target Specification:\n",
    "- OS: Ubuntu 22.04.4 LTS\n",
    "- Python: 3.11\n",
    "- Platform: Kaggle\n",
    "- CPU: Intel Xeon 8173M 2 Core / 4 Thread\n",
    "- RAM: 30GB\n",
    "- GPU: 2* Nvidia T4 15GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c52bf-a0db-4e13-a3da-989b1caa7431",
   "metadata": {},
   "source": [
    "Pre Training Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1099a3-6766-4c9e-8c7e-447f75525c54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T09:02:44.188241Z",
     "iopub.status.busy": "2025-11-04T09:02:44.187755Z",
     "iopub.status.idle": "2025-11-04T09:04:08.222656Z",
     "shell.execute_reply": "2025-11-04T09:04:08.221971Z",
     "shell.execute_reply.started": "2025-11-04T09:02:44.188218Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "%pip install onnxruntime-gpu pytorch-msssim\n",
    "\"\"\" if not Path(\"/kaggle/working/nv-tensorrt-local-repo-ubuntu2204-10.9.0-cuda-12.8_1.0-1_amd64.deb\").exists():\n",
    "    !sudo apt remove --purge -y 'libnvinfer*' 'tensorrt*' 'python3-libnvinfer*'\n",
    "    !sudo apt autoremove -y\n",
    "    !sudo apt clean\n",
    "    !sudo rm -rf /var/lib/apt/lists/*\n",
    "    %pip install onnxruntime-gpu pytorch-msssim tensorrt==10.9.0.34\n",
    "    !sudo sed -i '/developer\\.download\\.nvidia\\.com\\/compute\\/cuda/s/^/#/' /etc/apt/sources.list.d/*.list\n",
    "    !wget https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.9.0/local_repo/nv-tensorrt-local-repo-ubuntu2204-10.9.0-cuda-12.8_1.0-1_amd64.deb\n",
    "    !sudo dpkg -i nv-tensorrt-local-repo-ubuntu2204-10.9.0-cuda-12.8_1.0-1_amd64.deb\n",
    "    !sudo cp /var/nv-tensorrt-local-repo-ubuntu2204-10.9.0-cuda-12.8/*-keyring.gpg /usr/share/keyrings/\n",
    "    !echo \"deb [signed-by=/usr/share/keyrings/nv-tensorrt-local-AD7406A2-keyring.gpg] \\\n",
    "    file:/var/nv-tensorrt-local-repo-ubuntu2204-10.9.0-cuda-12.8 ./\" | \\\n",
    "    sudo tee /etc/apt/sources.list.d/tensorrt-local.list > /dev/null\n",
    "    !sudo apt update\n",
    "    !sudo apt install -y --allow-downgrades tensorrt=10.9.0* \\\n",
    "    libnvinfer10=10.9.0.* \\\n",
    "    libnvinfer-plugin10=10.9.0.* \\\n",
    "    libnvinfer-vc-plugin10=10.9.0.* \\\n",
    "    libnvinfer-lean10=10.9.0.* \\\n",
    "    libnvinfer-dispatch10=10.9.0.* \\\n",
    "    libnvonnxparsers10=10.9.0.* \\\n",
    "    python3-libnvinfer=10.9.0.* \\\n",
    "    tensorrt-libs=10.9.0.* \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29cbc29",
   "metadata": {
    "id": "e29cbc29"
   },
   "source": [
    "Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2675fdef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T09:04:15.462653Z",
     "iopub.status.busy": "2025-11-04T09:04:15.462404Z",
     "iopub.status.idle": "2025-11-04T09:04:15.474867Z",
     "shell.execute_reply": "2025-11-04T09:04:15.474141Z",
     "shell.execute_reply.started": "2025-11-04T09:04:15.462632Z"
    },
    "id": "2675fdef",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import onnxruntime as ort\n",
    "from pytorch_msssim import ssim\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor, ToPILImage, Compose, ColorJitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09162a59",
   "metadata": {
    "id": "09162a59"
   },
   "source": [
    "Training Config & Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e96d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T09:04:18.523200Z",
     "iopub.status.busy": "2025-11-04T09:04:18.522931Z",
     "iopub.status.idle": "2025-11-04T09:04:18.531824Z",
     "shell.execute_reply": "2025-11-04T09:04:18.531111Z",
     "shell.execute_reply.started": "2025-11-04T09:04:18.523182Z"
    },
    "id": "731e96d0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "UPSCALE_FACTOR = 2\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKER = os.cpu_count()\n",
    "EPOCHS = 150\n",
    "PATCH_SIZE = 256\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LOSS_ALPHA = 0.84\n",
    "BASE_DIR = Path(\".\")\n",
    "DATA_DIR = Path(\"/kaggle/input/datasetss\")\n",
    "OUTPUTS_DIR = BASE_DIR / \"outputs\"\n",
    "CHECKPOINTS_DIR = BASE_DIR / \"checkpoints\"\n",
    "CHECKPOINT_FILE = CHECKPOINTS_DIR / f\"best_espcn_x{UPSCALE_FACTOR}.pth\"\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Enabling cuDNN benchmark mode for GPU.\")\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806b70ea",
   "metadata": {
    "id": "806b70ea"
   },
   "source": [
    "Environment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4d01f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T09:04:22.503802Z",
     "iopub.status.busy": "2025-11-04T09:04:22.502993Z",
     "iopub.status.idle": "2025-11-04T09:04:22.512849Z",
     "shell.execute_reply": "2025-11-04T09:04:22.512237Z",
     "shell.execute_reply.started": "2025-11-04T09:04:22.503774Z"
    },
    "id": "4d4d01f6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_environment_and_datasets(patch_size):\n",
    "    CHECKPOINTS_DIR.mkdir(exist_ok=True)\n",
    "    OUTPUTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "    # Define a path for the cache file\n",
    "    CACHE_FILE_TRAIN = OUTPUTS_DIR / \"valid_train_paths.txt\"\n",
    "\n",
    "    # --- Handle Validation Paths (always scanned, it's fast) ---\n",
    "    valid_div2k_dir = DATA_DIR / \"DIV2K_valid_HR\"\n",
    "    validation_image_paths = glob.glob(str(valid_div2k_dir / '*.*'))\n",
    "\n",
    "    # --- Handle Training Paths (Check cache first) ---\n",
    "    valid_train_paths = []\n",
    "    if CACHE_FILE_TRAIN.exists():\n",
    "        print(f\"Loading cached training image paths from {CACHE_FILE_TRAIN}...\")\n",
    "        try:\n",
    "            with open(CACHE_FILE_TRAIN, 'r') as f:\n",
    "                valid_train_paths = [line.strip() for line in f if line.strip()]\n",
    "            print(f\"Loaded {len(valid_train_paths)} paths from cache.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not read cache file {e}. Re-building...\")\n",
    "            valid_train_paths = [] # Ensure list is empty to trigger re-build\n",
    "\n",
    "    if not valid_train_paths:\n",
    "        print(f\"Cache file not found or was invalid. Building new image list...\")\n",
    "        train_div2k_dir = DATA_DIR / \"DIV2K_train_HR\"\n",
    "        train_flickr2k_dir = DATA_DIR / \"Flickr2K_HR\"\n",
    "        personal_dir = DATA_DIR / \"Personal_HR\"\n",
    "\n",
    "        train_image_paths = glob.glob(str(train_div2k_dir / '*.*')) + \\\n",
    "                            glob.glob(str(train_flickr2k_dir / '*.*'))\n",
    "\n",
    "        if personal_dir.exists():\n",
    "            print(\"Personal dataset found. Adding to training set.\")\n",
    "            train_image_paths += glob.glob(str(personal_dir / '*.*'))\n",
    "        else:\n",
    "            print(\"Personal dataset not found. Proceeding without it.\")\n",
    "\n",
    "        print(f\"Found {len(train_image_paths)} potential training images.\")\n",
    "        print(f\"Verifying image dimensions against patch size {patch_size}...\")\n",
    "\n",
    "        def is_image_large_enough(image_path, min_size):\n",
    "            try:\n",
    "                img = cv2.imread(str(image_path))\n",
    "                if img is None:\n",
    "                    print(f\"Warning: Failed to load {image_path}. Skipping.\")\n",
    "                    return False\n",
    "                h, w = img.shape[:2]\n",
    "                return h >= min_size and w >= min_size\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error reading {image_path}: {e}. Skipping.\")\n",
    "                return False\n",
    "\n",
    "        valid_train_paths = [\n",
    "            p for p in tqdm(train_image_paths, desc=\"Filtering train images\")\n",
    "            if is_image_large_enough(p, patch_size)\n",
    "        ]\n",
    "\n",
    "        print(f\"Filtered training set: {len(valid_train_paths)} of {len(train_image_paths)} images remain.\")\n",
    "\n",
    "        # Write the new valid paths to the cache file\n",
    "        try:\n",
    "            print(f\"Saving new cache file to {CACHE_FILE_TRAIN}...\")\n",
    "            with open(CACHE_FILE_TRAIN, 'w') as f:\n",
    "                for path in valid_train_paths:\n",
    "                    f.write(f\"{path}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not write cache file: {e}\")\n",
    "\n",
    "    if not valid_train_paths or not validation_image_paths:\n",
    "        print(\"\\n--- VERIFICATION FAILED: No images found. Check your './datasets/' folder structure. ---\")\n",
    "\n",
    "    print(f\"--- Dataset paths prepared. Train: {len(valid_train_paths)}, Val: {len(validation_image_paths)} ---\")\n",
    "    return valid_train_paths, validation_image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a1cbcd",
   "metadata": {
    "id": "a0a1cbcd"
   },
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da2e21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T09:04:26.548389Z",
     "iopub.status.busy": "2025-11-04T09:04:26.547669Z",
     "iopub.status.idle": "2025-11-04T09:04:26.553881Z",
     "shell.execute_reply": "2025-11-04T09:04:26.553156Z",
     "shell.execute_reply.started": "2025-11-04T09:04:26.548363Z"
    },
    "id": "85da2e21",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.7):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        try:\n",
    "            self.ssim_loss_fn = ssim\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install pytorch-msssim: pip install pytorch-msssim\")\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # MSE Loss\n",
    "        loss_mse = self.mse_loss(output, target)\n",
    "\n",
    "        # SSIM Loss (1 - SSIM)\n",
    "        ssim_score = self.ssim_loss_fn(output, target, data_range=1.0, size_average=True)\n",
    "        loss_ssim = 1 - ssim_score\n",
    "\n",
    "        # Combined Loss\n",
    "        total_loss = self.alpha * loss_mse + (1 - self.alpha) * loss_ssim\n",
    "\n",
    "        return total_loss, loss_mse, ssim_score\n",
    "\n",
    "def psnr(mse):\n",
    "    return 10 * math.log10(1 / mse) if mse > 0 else float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6698a64d",
   "metadata": {
    "id": "6698a64d"
   },
   "source": [
    "Create Patches From HR Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f942143",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T09:04:30.053154Z",
     "iopub.status.busy": "2025-11-04T09:04:30.052441Z",
     "iopub.status.idle": "2025-11-04T09:04:30.064910Z",
     "shell.execute_reply": "2025-11-04T09:04:30.064123Z",
     "shell.execute_reply.started": "2025-11-04T09:04:30.053128Z"
    },
    "id": "2f942143",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TrainingSuperResolutionDataset(Dataset):\n",
    "    def __init__(self, image_filenames, crop_size, upscale_factor):\n",
    "        super(TrainingSuperResolutionDataset, self).__init__()\n",
    "        self.image_filenames = image_filenames\n",
    "        self.crop_size = crop_size - (crop_size % upscale_factor)\n",
    "        self.upscale_factor = upscale_factor\n",
    "        self.to_tensor = ToTensor()\n",
    "        self.scale_factor = 1 / upscale_factor\n",
    "        self.transform = Compose([\n",
    "            ToPILImage(),\n",
    "            ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            hr_image = cv2.imread(self.image_filenames[index])\n",
    "            if hr_image is None:\n",
    "                raise IOError(f\"cv2.imread failed to load image: {self.image_filenames[index]}\")\n",
    "            hr_image = cv2.cvtColor(hr_image, cv2.COLOR_BGR2RGB)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {self.image_filenames[index]}: {e}. Skipping.\")\n",
    "            return self.__getitem__((index + 1) % len(self))\n",
    "\n",
    "        h, w = hr_image.shape[:2]\n",
    "\n",
    "        i = random.randint(0, h - self.crop_size)\n",
    "        j = random.randint(0, w - self.crop_size)\n",
    "        hr_patch = hr_image[i:i+self.crop_size, j:j+self.crop_size, :]\n",
    "\n",
    "        if torch.rand(1) > 0.5:\n",
    "            hr_patch = cv2.flip(hr_patch, 1)\n",
    "\n",
    "        if torch.rand(1) > 0.5:\n",
    "            angle = random.choice([cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_180, cv2.ROTATE_90_COUNTERCLOCKWISE])\n",
    "            hr_patch = cv2.rotate(hr_patch, angle)\n",
    "\n",
    "        # --- START: Recommended Change ---\n",
    "\n",
    "        # 1. Create the CLEAN HR (target) tensor first\n",
    "        hr_patch_tensor = self.transform(hr_patch)\n",
    "\n",
    "        # 2. Create the CLEAN LR tensor from the clean HR tensor\n",
    "        lr_patch_tensor = F.interpolate(\n",
    "            hr_patch_tensor.unsqueeze(0),\n",
    "            scale_factor=self.scale_factor,\n",
    "            mode='bicubic',\n",
    "            align_corners=False,\n",
    "            antialias=True\n",
    "        ).squeeze(0)\n",
    "\n",
    "        # --- END: Recommended Change ---\n",
    "\n",
    "        return lr_patch_tensor, hr_patch_tensor # (Augmented_LR, Clean_HR)\n",
    "\n",
    "class ValidationSuperResolutionDataset(Dataset):\n",
    "    def __init__(self, image_filenames, upscale_factor):\n",
    "        super(ValidationSuperResolutionDataset, self).__init__()\n",
    "        self.image_filenames = image_filenames\n",
    "        self.upscale_factor = upscale_factor\n",
    "        self.to_tensor = ToTensor()\n",
    "        self.scale_factor = 1 / upscale_factor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            hr_image = cv2.imread(self.image_filenames[index])\n",
    "            if hr_image is None:\n",
    "                raise IOError(f\"cv2.imread failed to load image: {self.image_filenames[index]}\")\n",
    "            hr_image = cv2.cvtColor(hr_image, cv2.COLOR_BGR2RGB)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {self.image_filenames[index]}: {e}. Skipping.\")\n",
    "            return self.__getitem__((index + 1) % len(self))\n",
    "\n",
    "        h, w = hr_image.shape[:2]\n",
    "        w_new, h_new = w - (w % self.upscale_factor), h - (h % self.upscale_factor)\n",
    "        hr_image = hr_image[:h_new, :w_new, :]\n",
    "\n",
    "        hr_tensor = self.to_tensor(hr_image)\n",
    "\n",
    "        # This is the \"interpolasi bikubik\" method from your proposal \n",
    "        lr_tensor = F.interpolate(\n",
    "            hr_tensor.unsqueeze(0),\n",
    "            scale_factor=self.scale_factor,\n",
    "            mode='bicubic',\n",
    "            align_corners=False,\n",
    "            antialias=True\n",
    "        ).squeeze(0)\n",
    "        \n",
    "        return lr_tensor, hr_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e70967",
   "metadata": {
    "id": "a9e70967"
   },
   "source": [
    "Model Defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ccae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T09:04:34.824593Z",
     "iopub.status.busy": "2025-11-04T09:04:34.824313Z",
     "iopub.status.idle": "2025-11-04T09:04:34.829913Z",
     "shell.execute_reply": "2025-11-04T09:04:34.829269Z",
     "shell.execute_reply.started": "2025-11-04T09:04:34.824570Z"
    },
    "id": "3e5ccae0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ESPCN(nn.Module):\n",
    "    def __init__(self, upscale_factor):\n",
    "        super(ESPCN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 3 * (upscale_factor ** 2), kernel_size=3, padding=1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
    "        self.relu = nn.ReLU() \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pixel_shuffle(self.conv3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246210ca",
   "metadata": {},
   "source": [
    "automatically create ddp_worker.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3405fccf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T09:07:36.023326Z",
     "iopub.status.busy": "2025-11-04T09:07:36.023020Z",
     "iopub.status.idle": "2025-11-04T09:07:36.037464Z",
     "shell.execute_reply": "2025-11-04T09:07:36.036880Z",
     "shell.execute_reply.started": "2025-11-04T09:07:36.023304Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ddp_worker_code = \"\"\"\n",
    "# --- Library Import ---\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import torch.optim as optim\n",
    "from pytorch_msssim import ssim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torchvision.transforms import ToTensor, ToPILImage, Compose, ColorJitter\n",
    "\n",
    "# --- Training Config & Setting ---\n",
    "\n",
    "UPSCALE_FACTOR = 2\n",
    "BATCH_SIZE = 128 # Per GPU\n",
    "NUM_WORKER = os.cpu_count() // 2 # Per Process\n",
    "EPOCHS = 150\n",
    "PATCH_SIZE = 256\n",
    "LEARNING_RATE = 1e-4\n",
    "LOSS_ALPHA = 0.84\n",
    "BASE_DIR = Path(\".\") \n",
    "OUTPUTS_DIR = BASE_DIR / \"outputs\"\n",
    "CHECKPOINTS_DIR = BASE_DIR / \"checkpoints\"\n",
    "CHECKPOINT_FILE = CHECKPOINTS_DIR / f\"best_espcn_x{UPSCALE_FACTOR}.pth\"\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        # Note: This print will appear once per process\n",
    "        print(f\"Rank {dist.get_rank() if dist.is_initialized() else 0}: Enabling cuDNN benchmark mode for GPU.\")\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# --- Loss Function ---\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.7):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        try:\n",
    "            self.ssim_loss_fn = ssim\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install pytorch-msssim: pip install pytorch-msssim\")\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # MSE Loss\n",
    "        loss_mse = self.mse_loss(output, target)\n",
    "\n",
    "        # SSIM Loss (1 - SSIM)\n",
    "        ssim_score = self.ssim_loss_fn(output, target, data_range=1.0, size_average=True)\n",
    "        loss_ssim = 1 - ssim_score\n",
    "\n",
    "        # Combined Loss\n",
    "        total_loss = self.alpha * loss_mse + (1 - self.alpha) * loss_ssim\n",
    "\n",
    "        return total_loss, loss_mse, ssim_score\n",
    "\n",
    "def psnr(mse):\n",
    "    return 10 * math.log10(1 / mse) if mse > 0 else float('inf')\n",
    "\n",
    "# --- Create Patches From HR Images ---\n",
    "\n",
    "class TrainingSuperResolutionDataset(Dataset):\n",
    "    def __init__(self, image_filenames, crop_size, upscale_factor):\n",
    "        super(TrainingSuperResolutionDataset, self).__init__()\n",
    "        self.image_filenames = image_filenames\n",
    "        self.crop_size = crop_size - (crop_size % upscale_factor)\n",
    "        self.upscale_factor = upscale_factor\n",
    "        self.to_tensor = ToTensor()\n",
    "        self.scale_factor = 1 / upscale_factor\n",
    "        self.transform = Compose([\n",
    "            ToPILImage(),\n",
    "            ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            hr_image = cv2.imread(self.image_filenames[index])\n",
    "            if hr_image is None:\n",
    "                raise IOError(f\"cv2.imread failed to load image: {self.image_filenames[index]}\")\n",
    "            hr_image = cv2.cvtColor(hr_image, cv2.COLOR_BGR2RGB)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {self.image_filenames[index]}: {e}. Skipping.\")\n",
    "            # Recursively call __getitem__ with the next index, wrapping around\n",
    "            return self.__getitem__((index + 1) % len(self))\n",
    "\n",
    "        h, w = hr_image.shape[:2]\n",
    "\n",
    "        # Ensure image is large enough (should have been pre-filtered, but good safety check)\n",
    "        if h < self.crop_size or w < self.crop_size:\n",
    "             print(f\"Warning: Image {self.image_filenames[index]} is smaller than crop size. Skipping.\")\n",
    "             return self.__getitem__((index + 1) % len(self))\n",
    "\n",
    "        i = random.randint(0, h - self.crop_size)\n",
    "        j = random.randint(0, w - self.crop_size)\n",
    "        hr_patch = hr_image[i:i+self.crop_size, j:j+self.crop_size, :]\n",
    "\n",
    "        # Data Augmentation\n",
    "        if torch.rand(1) > 0.5:\n",
    "            hr_patch = cv2.flip(hr_patch, 1) # Horizontal flip\n",
    "\n",
    "        if torch.rand(1) > 0.5:\n",
    "            angle = random.choice([cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_180, cv2.ROTATE_90_COUNTERCLOCKWISE])\n",
    "            hr_patch = cv2.rotate(hr_patch, angle) # Rotations\n",
    "\n",
    "        # 1. Create the CLEAN HR (target) tensor first, applying color jitter\n",
    "        hr_patch_tensor = self.transform(hr_patch)\n",
    "\n",
    "        # 2. Create the CLEAN LR tensor from the clean HR tensor\n",
    "        lr_patch_tensor = F.interpolate(\n",
    "            hr_patch_tensor.unsqueeze(0),\n",
    "            scale_factor=self.scale_factor,\n",
    "            mode='bicubic',\n",
    "            align_corners=False,\n",
    "            antialias=True\n",
    "        ).squeeze(0)\n",
    "\n",
    "        return lr_patch_tensor, hr_patch_tensor # (Augmented_LR, Clean_HR)\n",
    "\n",
    "class ValidationSuperResolutionDataset(Dataset):\n",
    "    def __init__(self, image_filenames, upscale_factor):\n",
    "        super(ValidationSuperResolutionDataset, self).__init__()\n",
    "        self.image_filenames = image_filenames\n",
    "        self.upscale_factor = upscale_factor\n",
    "        self.to_tensor = ToTensor()\n",
    "        self.scale_factor = 1 / upscale_factor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            hr_image = cv2.imread(self.image_filenames[index])\n",
    "            if hr_image is None:\n",
    "                raise IOError(f\"cv2.imread failed to load image: {self.image_filenames[index]}\")\n",
    "            hr_image = cv2.cvtColor(hr_image, cv2.COLOR_BGR2RGB)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {self.image_filenames[index]}: {e}. Skipping.\")\n",
    "            return self.__getitem__((index + 1) % len(self))\n",
    "\n",
    "        h, w = hr_image.shape[:2]\n",
    "        # Crop to be divisible by upscale_factor\n",
    "        w_new, h_new = w - (w % self.upscale_factor), h - (h % self.upscale_factor)\n",
    "        hr_image = hr_image[:h_new, :w_new, :]\n",
    "\n",
    "        hr_tensor = self.to_tensor(hr_image)\n",
    "\n",
    "        # Create LR tensor using bicubic downsampling\n",
    "        lr_tensor = F.interpolate(\n",
    "            hr_tensor.unsqueeze(0),\n",
    "            scale_factor=self.scale_factor,\n",
    "            mode='bicubic',\n",
    "            align_corners=False,\n",
    "            antialias=True\n",
    "        ).squeeze(0)\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "# --- Model Definition ---\n",
    "\n",
    "class ESPCN(nn.Module):\n",
    "    def __init__(self, upscale_factor):\n",
    "        super(ESPCN, self).__init__()\n",
    "        # Kaggel notebook had 32, 16. The written file had 64, 64. Using 64, 64 as in the file.\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 3 * (upscale_factor ** 2), kernel_size=3, padding=1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
    "        self.relu = nn.ReLU() \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pixel_shuffle(self.conv3(x))\n",
    "        return x\n",
    "\n",
    "# --- Main Worker Function ---\n",
    "\n",
    "def main_worker(rank, world_size, train_paths, val_paths):\n",
    "    DEVICE = torch.device(f'cuda:{rank}')\n",
    "    torch.cuda.set_device(DEVICE)\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '42069' # Ensure this port is free\n",
    "    \n",
    "    # Increase timeout if needed, e.g., for slow startup\n",
    "    # import datetime\n",
    "    # timeout_delta = datetime.timedelta(minutes=30)\n",
    "    # dist.init_process_group(\"nccl\", rank=rank, world_size=world_size, timeout=timeout_delta)\n",
    "    \n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    set_seed(42 + rank) # Ensure different seed per process\n",
    "\n",
    "    if rank == 0: print(f\"Rank {rank}: Process group initialized.\")\n",
    "\n",
    "    train_dataset = TrainingSuperResolutionDataset(train_paths, crop_size=PATCH_SIZE, upscale_factor=UPSCALE_FACTOR)\n",
    "    val_dataset = ValidationSuperResolutionDataset(val_paths, upscale_factor=UPSCALE_FACTOR)\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True, seed=42)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, # Sampler handles shuffle\n",
    "        num_workers=min(4, NUM_WORKER), # Use fewer workers if CPU is limited\n",
    "        pin_memory=True, # Pin memory for faster GPU transfer\n",
    "        persistent_workers=(NUM_WORKER > 0 and min(4, NUM_WORKER) > 0), \n",
    "        worker_init_fn=seed_worker,\n",
    "        sampler=train_sampler, \n",
    "        drop_last=True # Important for DDP\n",
    "    )\n",
    "    \n",
    "    # Validation is only done on Rank 0\n",
    "    if rank == 0:\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, batch_size=1, shuffle=False, \n",
    "            num_workers=min(4, NUM_WORKER),\n",
    "            pin_memory=True,\n",
    "            persistent_workers=(NUM_WORKER > 0 and min(4, NUM_WORKER) > 0)\n",
    "        )\n",
    "        print(f\"Rank {rank}: DataLoaders created.\")\n",
    "\n",
    "    model = ESPCN(upscale_factor=UPSCALE_FACTOR).to(DEVICE)\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank], find_unused_parameters=False)\n",
    "    \n",
    "    if rank == 0: print(f\"Rank {rank}: Model wrapped in DDP.\")\n",
    "\n",
    "    criterion = CombinedLoss(alpha=LOSS_ALPHA).to(DEVICE)\n",
    "    # Scale LR by world size\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE * world_size)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "    scaler = GradScaler(enabled=True) # Enable AMP\n",
    "\n",
    "    start_epoch = 0\n",
    "    best_psnr = 0.0\n",
    "    log_file_path = OUTPUTS_DIR / 'training_log.csv'\n",
    "    training_log = []\n",
    "\n",
    "    # Load checkpoint only on Rank 0\n",
    "    if rank == 0:\n",
    "        CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        if CHECKPOINT_FILE.exists():\n",
    "            print(f\"Rank {rank}: Resuming from checkpoint: {CHECKPOINT_FILE}\")\n",
    "            checkpoint = torch.load(CHECKPOINT_FILE, map_location=DEVICE)\n",
    "\n",
    "            raw_state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "            # Handle potential '_orig_mod.' from torch.compile\n",
    "            needs_compile_strip = any(key.startswith('_orig_mod.') for key in raw_state_dict.keys())\n",
    "            if needs_compile_strip:\n",
    "                 print(\"  Stripping '_orig_mod.' prefix from checkpoint...\")\n",
    "                 stripped_dict = {k.replace('_orig_mod.', ''): v for k, v in raw_state_dict.items()}\n",
    "                 raw_state_dict = stripped_dict\n",
    "\n",
    "            model.module.load_state_dict(raw_state_dict)\n",
    "\n",
    "            try: \n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            except KeyError:\n",
    "                 print(\"Warning: Optimizer/Scheduler state not found in checkpoint. Reinitializing.\")\n",
    "\n",
    "            start_epoch = checkpoint.get('epoch', 0) + 1\n",
    "            best_psnr = checkpoint.get('best_psnr', 0.0)\n",
    "            print(f\"Rank {rank}: Resumed from epoch {start_epoch}, best PSNR {best_psnr:.4f} dB.\")\n",
    "\n",
    "            if log_file_path.exists():\n",
    "                try:\n",
    "                    log_df = pd.read_csv(log_file_path)\n",
    "                    log_df = log_df[log_df['epoch'] < start_epoch]\n",
    "                    training_log = log_df.to_dict('records')\n",
    "                    print(f\"Rank {rank}: Loaded {len(training_log)} previous log entries.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Rank {rank}: Warn: Log load failed: {e}. Starting empty log.\")\n",
    "                    training_log = []\n",
    "            print(f\"Rank {rank}: Checkpoint loaded successfully.\")\n",
    "        else:\n",
    "            print(f\"Rank {rank}: No checkpoint found. Starting fresh.\")\n",
    "            if log_file_path.exists():\n",
    "                print(f\"Rank {rank}: Deleting old log file: {log_file_path}\")\n",
    "                log_file_path.unlink()\n",
    "            training_log = []\n",
    "\n",
    "    # --- Synchronization ---\n",
    "    if rank == 0: print(f\"Rank {rank}: Starting synchronization...\")\n",
    "\n",
    "    # Sync start_epoch and best_psnr from rank 0 to others\n",
    "    sync_list = [start_epoch, best_psnr]\n",
    "    dist.broadcast_object_list(sync_list, src=0)\n",
    "    start_epoch, best_psnr = sync_list[0], sync_list[1]\n",
    "\n",
    "    # Sync model parameters from rank 0\n",
    "    for param in model.parameters():\n",
    "        dist.broadcast(param.data, src=0)\n",
    "\n",
    "    # ========== START: CORRECTED OPTIMIZER SYNC ==========\n",
    "    # Sync optimizer state (This is the correct way)\n",
    "    # 1. All ranks create a list. On rank 0, it contains the loaded state.\n",
    "    #    On other ranks, it's just a placeholder based on their empty optimizer.\n",
    "    optimizer_state_list = [optimizer.state_dict()]\n",
    "    \n",
    "    # 2. Broadcast the list from rank 0 to all other ranks.\n",
    "    dist.broadcast_object_list(optimizer_state_list, src=0)\n",
    "    \n",
    "    # 3. All processes (including rank 0, harmlessly) load the state\n",
    "    #    that just came from rank 0.\n",
    "    optimizer.load_state_dict(optimizer_state_list[0])\n",
    "    # ========== END: CORRECTED OPTIMIZER SYNC ==========\n",
    "\n",
    "    # Sync scheduler state (last_epoch is crucial)\n",
    "    scheduler_state_list = [scheduler.state_dict()]\n",
    "    dist.broadcast_object_list(scheduler_state_list, src=0)\n",
    "    scheduler.load_state_dict(scheduler_state_list[0])\n",
    "\n",
    "    if rank == 0: print(\"Rank 0: All processes synchronized.\")\n",
    "    dist.barrier() # Ensure everyone is ready\n",
    "    if rank == 0: print(f\"Rank {rank}: Passed synchronization barrier.\")\n",
    "\n",
    "    # --- torch.compile ---\n",
    "    compiled_model = None\n",
    "    try:\n",
    "        if rank == 0: print(f\"Rank {rank}: Attempting torch.compile...\")\n",
    "        compiled_model = torch.compile(model)\n",
    "        if rank == 0: print(f\"Rank {rank}: torch.compile successful.\")\n",
    "    except Exception as e:\n",
    "        if rank == 0: print(f\"Rank {rank}: torch.compile() failed: {e}. Running eager.\")\n",
    "        compiled_model = model # Fallback to non-compiled model\n",
    "\n",
    "    model_to_train = compiled_model\n",
    "\n",
    "    if rank == 0:\n",
    "        base_model = model_to_train.module\n",
    "        if hasattr(base_model, '_orig_mod'):\n",
    "             base_model = base_model._orig_mod\n",
    "        num_params = sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
    "        print(f\"--- Training ESPCN Model ({num_params:,} parameters) ---\")\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    start_time = time.time()\n",
    "    if rank == 0: print(f\"Rank {rank}: Starting training loop from epoch {start_epoch + 1}...\")\n",
    "\n",
    "    for epoch in range(start_epoch, EPOCHS):\n",
    "        model_to_train.train()\n",
    "        train_sampler.set_epoch(epoch)\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", disable=(rank != 0))\n",
    "        epoch_losses = [] \n",
    "\n",
    "        for batch_idx, (lr_images, hr_images) in enumerate(progress_bar):\n",
    "            lr_images = lr_images.to(DEVICE, non_blocking=True)\n",
    "            hr_images = hr_images.to(DEVICE, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with autocast(device_type=DEVICE.type, enabled=True):\n",
    "                outputs = model_to_train(lr_images)\n",
    "                total_loss, mse, ssim_score = criterion(outputs.float(), hr_images.float())\n",
    "\n",
    "            if torch.isnan(total_loss):\n",
    "                print(f\"Rank {rank}, Epoch {epoch+1}, Batch {batch_idx}: NaN loss detected! Skipping batch.\")\n",
    "                continue \n",
    "\n",
    "            scaler.scale(total_loss).backward()\n",
    "            \n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model_to_train.parameters(), max_norm=1.0)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            if rank == 0:\n",
    "                epoch_losses.append(total_loss.item())\n",
    "                progress_bar.set_postfix(\n",
    "                    Loss=f\"{total_loss.item():.4f}\",\n",
    "                    MSE=f\"{mse.item():.4f}\",\n",
    "                    SSIM=f\"{ssim_score.item():.4f}\",\n",
    "                    LR=f\"{scheduler.get_last_lr()[0]:.1E}\"\n",
    "                )\n",
    "\n",
    "        # --- Validation (Rank 0) ---\n",
    "        avg_val_psnr, avg_val_ssim = 0.0, 0.0\n",
    "        needs_save = False \n",
    "\n",
    "        if rank == 0:\n",
    "            model_to_train.eval()\n",
    "            val_psnr_list = []\n",
    "            val_ssim_list = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                val_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\", leave=False)\n",
    "                for i, (lr_val, hr_val) in enumerate(val_progress):\n",
    "                    lr_val, hr_val = lr_val.to(DEVICE), hr_val.to(DEVICE)\n",
    "\n",
    "                    eval_model = model_to_train.module\n",
    "                    if hasattr(eval_model, '_orig_mod'):\n",
    "                         eval_model = eval_model._orig_mod\n",
    "\n",
    "                    with autocast(device_type=DEVICE.type, enabled=True):\n",
    "                        output_val = eval_model(lr_val)\n",
    "                    \n",
    "                    output_val = output_val.float() # Ensure float32 for metric calculation\n",
    "                    hr_val = hr_val.float()\n",
    "\n",
    "                    mse_val = F.mse_loss(output_val, hr_val)\n",
    "                    if not torch.isnan(mse_val):\n",
    "                        val_psnr_list.append(psnr(mse_val.item()))\n",
    "                    val_ssim_list.append(ssim(output_val, hr_val, data_range=1.0, size_average=True).item())\n",
    "\n",
    "                    if i == 0 and (epoch + 1) % 5 == 0:\n",
    "                        try:\n",
    "                            # Clamp output for image saving\n",
    "                            sr_pil = ToPILImage()(output_val.squeeze(0).cpu().clamp(0.0, 1.0))\n",
    "                            output_path = OUTPUTS_DIR / f'val_epoch_{epoch+1}.png'\n",
    "                            sr_pil.save(output_path)\n",
    "                        except Exception as img_e:\n",
    "                             print(f\"Rank {rank}: Failed to save validation image: {img_e}\")\n",
    "\n",
    "            avg_val_psnr = np.mean(val_psnr_list) if val_psnr_list else 0.0\n",
    "            avg_val_ssim = np.mean(val_ssim_list) if val_ssim_list else 0.0\n",
    "            avg_epoch_loss = np.mean(epoch_losses) if epoch_losses else 0.0\n",
    "\n",
    "            print(f\"\\\\nEpoch {epoch+1} Summary (Rank 0):\")\n",
    "            print(f\"  - Avg Train Loss: {avg_epoch_loss:.4f}\")\n",
    "            print(f\"  - Val PSNR: {avg_val_psnr:.4f} dB | Val SSIM: {avg_val_ssim:.4f}\")\n",
    "            print(f\"  - Current LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "            training_log.append({ 'epoch': epoch + 1, 'train_loss': avg_epoch_loss, 'val_psnr': avg_val_psnr, 'val_ssim': avg_val_ssim, 'learning_rate': scheduler.get_last_lr()[0] })\n",
    "            try:\n",
    "                log_df = pd.DataFrame(training_log); log_df.to_csv(log_file_path, index=False)\n",
    "            except Exception as log_e:\n",
    "                 print(f\"Rank {rank}: Failed to save log: {log_e}\")\n",
    "\n",
    "            if avg_val_psnr > best_psnr:\n",
    "                best_psnr = avg_val_psnr\n",
    "                needs_save = True \n",
    "                print(f\"  - New best model! PSNR: {best_psnr:.4f} dB. Preparing to save...\")\n",
    "\n",
    "        # --- Checkpoint Saving (Rank 0, if needed) ---\n",
    "        save_info = [needs_save, best_psnr]\n",
    "        dist.broadcast_object_list(save_info, src=0)\n",
    "        needs_save, best_psnr = save_info[0], save_info[1] # Update best_psnr on all ranks\n",
    "\n",
    "        if needs_save and rank == 0:\n",
    "             print(f\"  Rank {rank}: Saving checkpoint for epoch {epoch+1}...\") # Corrected epoch num\n",
    "             try:\n",
    "                 save_model = model_to_train.module\n",
    "                 if hasattr(save_model, '_orig_mod'):\n",
    "                      save_model = save_model._orig_mod\n",
    "\n",
    "                 model_state_to_save = save_model.state_dict()\n",
    "\n",
    "                 torch.save({\n",
    "                     'epoch': epoch, 'model_state_dict': model_state_to_save,\n",
    "                     'optimizer_state_dict': optimizer.state_dict(),\n",
    "                     'scheduler_state_dict': scheduler.state_dict(), 'best_psnr': best_psnr,\n",
    "                 }, CHECKPOINT_FILE)\n",
    "                 print(f\"  Rank {rank}: Checkpoint saved successfully.\")\n",
    "             except Exception as save_e:\n",
    "                  print(f\"Rank {rank}: ERROR saving checkpoint: {save_e}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        dist.barrier()\n",
    "\n",
    "    # --- End of Training ---\n",
    "    dist.barrier() \n",
    "    if rank == 0:\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\\\nTraining complete in {total_time/60:.2f} minutes ({total_time/3600:.2f} hours).\")\n",
    "        print(f\"Best model saved to '{CHECKPOINT_FILE}' (PSNR: {best_psnr:.4f} dB).\")\n",
    "        print(f\"Log saved to {log_file_path}\")\n",
    "\n",
    "        try:\n",
    "             print(\"Generating final training graphs...\")\n",
    "             log_df = pd.DataFrame(training_log)\n",
    "             if not log_df.empty:\n",
    "                 plt.figure(figsize=(12, 5))\n",
    "                 plt.subplot(1, 2, 1); plt.plot(log_df['epoch'], log_df['val_psnr'], marker='o'); plt.title('Validation PSNR vs. Epoch'); plt.xlabel('Epoch'); plt.ylabel('PSNR (dB)'); plt.grid(True); plt.tight_layout()\n",
    "                 plt.subplot(1, 2, 2); plt.plot(log_df['epoch'], log_df['val_ssim'], marker='o', color='g'); plt.title('Validation SSIM vs. Epoch'); plt.xlabel('Epoch'); plt.ylabel('SSIM'); plt.grid(True); plt.tight_layout()\n",
    "                 plt.savefig(OUTPUTS_DIR / 'training_plots.png')\n",
    "             else:\n",
    "                  print(\"Log DataFrame is empty, skipping plot generation.\")\n",
    "        except Exception as plot_e:\n",
    "             print(f\"Error generating plots: {plot_e}\")\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "\"\"\"\n",
    "\n",
    "# Write the code to the file\n",
    "try:\n",
    "    with open(\"ddp_worker.py\", \"w\") as f:\n",
    "        f.write(ddp_worker_code)\n",
    "    print(\"Successfully wrote ddp_worker.py\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing ddp_worker.py: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d250a4ae",
   "metadata": {
    "id": "d250a4ae"
   },
   "source": [
    "Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197dcee6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T09:07:40.763811Z",
     "iopub.status.busy": "2025-11-04T09:07:40.763280Z"
    },
    "id": "197dcee6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Import the worker function from the file we just created ---\n",
    "try:\n",
    "    from ddp_worker import main_worker\n",
    "    print(\"Successfully imported main_worker from ddp_worker.py\")\n",
    "except ImportError as e:\n",
    "    print(f\"Could not import main_worker: {e}\")\n",
    "    # Optionally raise an error or exit if import fails\n",
    "    raise ImportError(\"Failed to load the worker function. Check ddp_worker.py.\") from e\n",
    "\n",
    "# --- The main_worker function definition is REMOVED from this cell ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure multiprocessing start method is appropriate for CUDA (forkserver or spawn recommended)\n",
    "    # Kaggle might default correctly, but explicitly setting can help avoid issues.\n",
    "    try:\n",
    "        mp.set_start_method('spawn', force=True) # 'spawn' is generally safer with CUDA\n",
    "        print(\"Set multiprocessing start method to 'spawn'.\")\n",
    "    except RuntimeError:\n",
    "        print(\"Multiprocessing context already set.\") # Ignore if already set\n",
    "\n",
    "    # 1. Prepare datasets *once* in the main process\n",
    "    prep_start_time = time.time()\n",
    "    print(\"--- Preparing Datasets ---\")\n",
    "    # Make sure prepare_environment_and_datasets is defined in a cell ABOVE this one\n",
    "    train_paths, val_paths = prepare_environment_and_datasets(PATCH_SIZE)\n",
    "    if not train_paths or not val_paths:\n",
    "        raise RuntimeError(\"Dataset paths not found. Halting execution.\")\n",
    "    print(f\"--- Dataset preparation complete in {time.time() - prep_start_time:.2f} seconds ---\")\n",
    "\n",
    "    # 2. Check for GPU and decide on world_size\n",
    "    world_size = torch.cuda.device_count()\n",
    "    if world_size > 1:\n",
    "        print(f\"Found {world_size} GPUs. Spawning DDP processes...\")\n",
    "        # Note: Pass paths as absolute strings to avoid issues in spawned processes\n",
    "        abs_train_paths = [str(Path(p).resolve()) for p in train_paths]\n",
    "        abs_val_paths = [str(Path(p).resolve()) for p in val_paths]\n",
    "        \n",
    "        process_context = mp.spawn(\n",
    "            main_worker,\n",
    "            args=(world_size, abs_train_paths, abs_val_paths), # Pass absolute paths\n",
    "            nprocs=world_size,\n",
    "            join=False # Set join=False initially to potentially catch errors sooner\n",
    "        )\n",
    "        try:\n",
    "            # Wait for processes to complete\n",
    "            process_context.join()\n",
    "            print(\"All DDP processes finished.\")\n",
    "            # Check exit codes if needed (0 means success)\n",
    "            # for i, exitcode in enumerate(process_context.exitcodes):\n",
    "            #     if exitcode != 0:\n",
    "            #         print(f\"Warning: Process {i} exited with code {exitcode}\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"KeyboardInterrupt received, attempting to terminate processes...\")\n",
    "            # Handle manual interruption if needed\n",
    "            for p in process_context.processes:\n",
    "                if p.is_alive():\n",
    "                    p.terminate()\n",
    "            process_context.join() # Wait for termination\n",
    "            print(\"Processes terminated.\")\n",
    "        except Exception as spawn_e:\n",
    "             print(f\"An error occurred during DDP execution: {spawn_e}\")\n",
    "             # Ensure processes are cleaned up on error\n",
    "             for p in process_context.processes:\n",
    "                 if p.is_alive():\n",
    "                     p.terminate()\n",
    "             process_context.join()\n",
    "             raise spawn_e # Re-raise the exception\n",
    "\n",
    "    elif world_size == 1:\n",
    "        print(\"Found 1 GPU. Running in single-process mode (no spawn needed).\")\n",
    "        # Run directly in the main process for simplicity if only 1 GPU\n",
    "        try:\n",
    "             # Pass paths directly\n",
    "             main_worker(0, 1, train_paths, val_paths)\n",
    "        except KeyboardInterrupt:\n",
    "             print(\"Training interrupted.\")\n",
    "        except Exception as e:\n",
    "             print(f\"An error occurred during single-GPU execution: {e}\")\n",
    "             raise e\n",
    "    else:\n",
    "        print(\"No GPU found. Running on CPU (not recommended).\")\n",
    "        try:\n",
    "             main_worker(0, 1, train_paths, val_paths) # Will run on CPU\n",
    "        except KeyboardInterrupt:\n",
    "             print(\"Training interrupted.\")\n",
    "        except Exception as e:\n",
    "             print(f\"An error occurred during CPU execution: {e}\")\n",
    "             raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1KSQx28dzQI",
   "metadata": {
    "id": "e1KSQx28dzQI"
   },
   "source": [
    "Convert To FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OT6qOk-Ad2xu",
   "metadata": {
    "id": "OT6qOk-Ad2xu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"--- Applying FP16 Quantization ---\")\n",
    "\n",
    "quant_model = ESPCN(upscale_factor=UPSCALE_FACTOR).to(DEVICE)\n",
    "checkpoint_path = CHECKPOINTS_DIR / f\"best_espcn_x{UPSCALE_FACTOR}.pth\"\n",
    "checkpoint = torch.load(CHECKPOINT_FILE, map_location=DEVICE)\n",
    "\n",
    "# <<<--- ADD THIS BLOCK --- >>>\n",
    "original_state_dict = checkpoint['model_state_dict']\n",
    "new_state_dict = {}\n",
    "needs_prefix_stripping = any(key.startswith('_orig_mod.') for key in original_state_dict.keys())\n",
    "if needs_prefix_stripping:\n",
    "    print(\"Detected '_orig_mod.' prefix from torch.compile(). Stripping prefix for FP16 conversion...\")\n",
    "    for key, value in original_state_dict.items():\n",
    "        if key.startswith('_orig_mod.'):\n",
    "            new_key = key[len('_orig_mod.'):] # Remove the prefix\n",
    "            new_state_dict[new_key] = value\n",
    "        else:\n",
    "            new_state_dict[key] = value # Keep keys without the prefix as is\n",
    "else:\n",
    "     print(\"No '_orig_mod.' prefix detected. Loading state dict as is for FP16 conversion.\")\n",
    "     new_state_dict = original_state_dict # Use the original if no prefix found\n",
    "\n",
    "quant_model.load_state_dict(new_state_dict) # <<<--- LOAD THE STRIPPED DICT\n",
    "# <<<--- END OF ADDED BLOCK --- >>>\n",
    "\n",
    "quant_model.eval()\n",
    "# Note: .half() is usually for GPU. On CPU, it might be slow or unsupported.\n",
    "# Consider keeping it FP32 for CPU-only export unless you have specific needs.\n",
    "# If you keep .half(), ensure subsequent ONNX export/benchmark handles FP16.\n",
    "quant_model.half()\n",
    "\n",
    "print(\"Model converted to FP16.\")\n",
    "\n",
    "quantized_model_path = CHECKPOINTS_DIR / f\"best_espcn_x{UPSCALE_FACTOR}_fp16.pth\"\n",
    "# Saving the state_dict directly is correct here\n",
    "torch.save(quant_model.state_dict(), quantized_model_path)\n",
    "\n",
    "print(f\"FP16 quantized model saved to: {quantized_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nT7gxmzLgwrN",
   "metadata": {
    "id": "nT7gxmzLgwrN"
   },
   "source": [
    "Export To ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vXI9Iz4XgyAN",
   "metadata": {
    "id": "vXI9Iz4XgyAN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"--- Exporting to ONNX ---\")\n",
    "\n",
    "EXPORT_FP16 = True\n",
    "\n",
    "if EXPORT_FP16:\n",
    "    print(\"Exporting FP16 model...\")\n",
    "    onnx_model = ESPCN(upscale_factor=UPSCALE_FACTOR)\n",
    "    checkpoint_path_fp16 = CHECKPOINTS_DIR / f\"best_espcn_x{UPSCALE_FACTOR}_fp16.pth\"\n",
    "    onnx_model.load_state_dict(torch.load(checkpoint_path_fp16))\n",
    "    onnx_model.eval().half().to(DEVICE)\n",
    "    dummy_input = torch.randn(1, 3, 720, 1280, device=DEVICE).half()\n",
    "    onnx_model_path = OUTPUTS_DIR / f\"espcn_x{UPSCALE_FACTOR}_fp16.onnx\"\n",
    "else:\n",
    "    print(\"Exporting FP32 model...\")\n",
    "    onnx_model = ESPCN(upscale_factor=UPSCALE_FACTOR).to(DEVICE)\n",
    "    checkpoint_path = CHECKPOINTS_DIR / f\"best_espcn_x{UPSCALE_FACTOR}.pth\"\n",
    "    checkpoint = torch.load(CHECKPOINT_FILE, map_location=DEVICE)\n",
    "\n",
    "    # <<< --- START: ADD THIS FIX --- >>>\n",
    "    original_state_dict = checkpoint['model_state_dict']\n",
    "    new_state_dict = {}\n",
    "    needs_prefix_stripping = any(key.startswith('_orig_mod.') for key in original_state_dict.keys())\n",
    "\n",
    "    if needs_prefix_stripping:\n",
    "        print(\"Detected '_orig_mod.' prefix. Stripping for ONNX export...\")\n",
    "        for key, value in original_state_dict.items():\n",
    "            if key.startswith('_orig_mod.'):\n",
    "                new_key = key[len('_orig_mod.'):] # Remove the prefix\n",
    "                new_state_dict[new_key] = value\n",
    "            else:\n",
    "                new_state_dict[key] = value\n",
    "    else:\n",
    "         print(\"No '_orig_mod.' prefix detected. Loading state dict as is for ONNX export.\")\n",
    "         new_state_dict = original_state_dict\n",
    "\n",
    "    onnx_model.load_state_dict(new_state_dict) # <<< --- LOAD THE STRIPPED DICT\n",
    "    # <<< --- END: ADD THIS FIX --- >>>\n",
    "\n",
    "    onnx_model.eval()\n",
    "    dummy_input = torch.randn(1, 3, 720, 1280, device=DEVICE)\n",
    "    onnx_model_path = OUTPUTS_DIR / f\"espcn_x{UPSCALE_FACTOR}.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    onnx_model,\n",
    "    dummy_input,\n",
    "    str(onnx_model_path),\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamo=True,\n",
    "    dynamic_axes={\n",
    "        'input': {2: 'height', 3: 'width'},\n",
    "        'output': {2: 'height_out', 3: 'width_out'}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Model successfully exported to ONNX: {onnx_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jnf5DCUhhzop",
   "metadata": {
    "id": "jnf5DCUhhzop"
   },
   "source": [
    "Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vaqAOnrmh1Fo",
   "metadata": {
    "id": "vaqAOnrmh1Fo",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Running Inference Benchmark ---\")\n",
    "\n",
    "if EXPORT_FP16:\n",
    "    test_image_np = np.random.randn(1, 3, 720, 1280).astype(np.float16)\n",
    "else:\n",
    "    test_image_np = np.random.randn(1, 3, 720, 1280).astype(np.float32)\n",
    "test_image_torch = torch.from_numpy(test_image_np).to(DEVICE)\n",
    "\n",
    "if EXPORT_FP16:\n",
    "    test_image_torch = test_image_torch.half()\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = onnx_model(test_image_torch)\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = onnx_model(test_image_torch)\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    pytorch_time = (time.time() - start_time) / 100\n",
    "\n",
    "print(f\"PyTorch Inference Time: {pytorch_time * 1000:.4f} ms per image\")\n",
    "\n",
    "providers = [\n",
    "    # ==> Tier 1: Highest-Performance, Hardware-Specific Providers (NVIDIA, AMD, Intel, Apple)\n",
    "    'TensorrtExecutionProvider',      # NVIDIA's top-tier for speed\n",
    "    'CUDAExecutionProvider',          # Standard NVIDIA GPU provider\n",
    "    'MIGraphXExecutionProvider',      # AMD's high-performance graph compiler on Linux\n",
    "    'ROCmExecutionProvider',          # Standard AMD GPU provider on Linux\n",
    "    'OpenVINOExecutionProvider',      # Optimized for Intel GPUs and CPUs\n",
    "    'CoreMLExecutionProvider',        # For Apple M-series chips (macOS, iOS)\n",
    "\n",
    "    # ==> Tier 2: Specialized NPU/Edge/Mobile Providers\n",
    "    'QNNExecutionProvider',           # Qualcomm AI Engine Direct (Snapdragon)\n",
    "    'NNAPIExecutionProvider',         # Android Neural Networks API\n",
    "    'CANNExecutionProvider',          # Huawei Ascend Chips\n",
    "    'RockchipNpuExecutionProvider',   # Rockchip NPUs\n",
    "    'VitisAIExecutionProvider',       # Xilinx FPGAs\n",
    "    'ArmNNExecutionProvider',         # Arm NN SDK\n",
    "    'ACLExecutionProvider',           # Arm Compute Library\n",
    "\n",
    "    # ==> Tier 3: General-Purpose GPU Provider (Windows)\n",
    "    'DmlExecutionProvider',           # DirectX 12 for NVIDIA, AMD, Intel GPUs on Windows\n",
    "\n",
    "    # ==> Tier 4: Optimized CPU Providers\n",
    "    'DnnlExecutionProvider',          # Intel's high-performance DNNL for CPUs\n",
    "    'XnnpackExecutionProvider',       # Optimized for ARM CPUs\n",
    "\n",
    "    # ==> Tier 5: Advanced Compiler & Cloud Providers\n",
    "    'TvmExecutionProvider',           # Apache TVM\n",
    "    'AzureExecutionProvider',         # For running on Azure\n",
    "\n",
    "    # ==> Tier 6: Default Fallback\n",
    "    'CPUExecutionProvider',           # The universal fallback that runs on any CPU\n",
    "]\n",
    "\n",
    "session = ort.InferenceSession(str(onnx_model_path), providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "ort_inputs = {session.get_inputs()[0].name: test_image_np}\n",
    "_ = session.run(None, ort_inputs)\n",
    "start_time = time.time()\n",
    "for _ in range(100):\n",
    "    _ = session.run(None, ort_inputs)\n",
    "onnx_time = (time.time() - start_time) / 100\n",
    "\n",
    "print(f\"ONNX Runtime Inference Time: {onnx_time * 1000:.4f} ms per image\")\n",
    "\n",
    "if pytorch_time > 0:\n",
    "    speed_increase = (pytorch_time - onnx_time) / pytorch_time * 100\n",
    "\n",
    "    print(f\"\\nResult: ONNX Runtime is ~{speed_increase:.2f}% faster than PyTorch for this model.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "14AB3UkpiuPOQecEDK3iAB5PhTfQLrrnd",
     "timestamp": 1760865764796
    }
   ]
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8634943,
     "sourceId": 13590539,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
